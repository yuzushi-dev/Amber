============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- /home/daniele/Amber_2.0/.venv/bin/python3
cachedir: .pytest_cache
rootdir: /home/daniele/Amber_2.0
configfile: pytest.ini (WARNING: ignoring pytest config in pyproject.toml!)
plugins: locust-2.43.0, anyio-4.12.0, cov-7.0.0, asyncio-1.3.0
asyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=function, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests/integration/test_ingestion_pipeline.py::TestIngestionPipeline::test_complete_pipeline FAILED [100%]

=================================== FAILURES ===================================
_________________ TestIngestionPipeline.test_complete_pipeline _________________

    @contextlib.contextmanager
    def map_httpcore_exceptions() -> typing.Iterator[None]:
        global HTTPCORE_EXC_MAP
        if len(HTTPCORE_EXC_MAP) == 0:
            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()
        try:
>           yield

.venv/lib/python3.12/site-packages/httpx/_transports/default.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncHTTPTransport object at 0x7eb5fbf3ec00>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>

    async def handle_async_request(
        self,
        request: Request,
    ) -> Response:
        assert isinstance(request.stream, AsyncByteStream)
        import httpcore
    
        req = httpcore.Request(
            method=request.method,
            url=httpcore.URL(
                scheme=request.url.raw_scheme,
                host=request.url.raw_host,
                port=request.url.port,
                target=request.url.raw_path,
            ),
            headers=request.headers.raw,
            content=request.stream,
            extensions=request.extensions,
        )
        with map_httpcore_exceptions():
>           resp = await self._pool.handle_async_request(req)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncConnectionPool [Requests: 0 active, 0 queued | Connections: 0 active, 0 idle]>
request = <Request [b'POST']>

    async def handle_async_request(self, request: Request) -> Response:
        """
        Send an HTTP request, and return an HTTP response.
    
        This is the core implementation that is called into by `.request()` or `.stream()`.
        """
        scheme = request.url.scheme.decode()
        if scheme == "":
            raise UnsupportedProtocol(
                "Request URL is missing an 'http://' or 'https://' protocol."
            )
        if scheme not in ("http", "https", "ws", "wss"):
            raise UnsupportedProtocol(
                f"Request URL has an unsupported protocol '{scheme}://'."
            )
    
        timeouts = request.extensions.get("timeout", {})
        timeout = timeouts.get("pool", None)
    
        with self._optional_thread_lock:
            # Add the incoming request to our request queue.
            pool_request = AsyncPoolRequest(request)
            self._requests.append(pool_request)
    
        try:
            while True:
                with self._optional_thread_lock:
                    # Assign incoming requests to available connections,
                    # closing or creating new connections as required.
                    closing = self._assign_requests_to_connections()
                await self._close_connections(closing)
    
                # Wait until this request has an assigned connection.
                connection = await pool_request.wait_for_connection(timeout=timeout)
    
                try:
                    # Send the request on the assigned connection.
                    response = await connection.handle_async_request(
                        pool_request.request
                    )
                except ConnectionNotAvailable:
                    # In some cases a connection may initially be available to
                    # handle a request, but then become unavailable.
                    #
                    # In this case we clear the connection and try again.
                    pool_request.clear_connection()
                else:
                    break  # pragma: nocover
    
        except BaseException as exc:
            with self._optional_thread_lock:
                # For any exception or cancellation we remove the request from
                # the queue, and then re-assign requests to connections.
                self._requests.remove(pool_request)
                closing = self._assign_requests_to_connections()
    
            await self._close_connections(closing)
>           raise exc from None

.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncConnectionPool [Requests: 0 active, 0 queued | Connections: 0 active, 0 idle]>
request = <Request [b'POST']>

    async def handle_async_request(self, request: Request) -> Response:
        """
        Send an HTTP request, and return an HTTP response.
    
        This is the core implementation that is called into by `.request()` or `.stream()`.
        """
        scheme = request.url.scheme.decode()
        if scheme == "":
            raise UnsupportedProtocol(
                "Request URL is missing an 'http://' or 'https://' protocol."
            )
        if scheme not in ("http", "https", "ws", "wss"):
            raise UnsupportedProtocol(
                f"Request URL has an unsupported protocol '{scheme}://'."
            )
    
        timeouts = request.extensions.get("timeout", {})
        timeout = timeouts.get("pool", None)
    
        with self._optional_thread_lock:
            # Add the incoming request to our request queue.
            pool_request = AsyncPoolRequest(request)
            self._requests.append(pool_request)
    
        try:
            while True:
                with self._optional_thread_lock:
                    # Assign incoming requests to available connections,
                    # closing or creating new connections as required.
                    closing = self._assign_requests_to_connections()
                await self._close_connections(closing)
    
                # Wait until this request has an assigned connection.
                connection = await pool_request.wait_for_connection(timeout=timeout)
    
                try:
                    # Send the request on the assigned connection.
>                   response = await connection.handle_async_request(
                        pool_request.request
                    )

.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncHTTPConnection [CONNECTION FAILED]>, request = <Request [b'POST']>

    async def handle_async_request(self, request: Request) -> Response:
        if not self.can_handle_request(request.url.origin):
            raise RuntimeError(
                f"Attempted to send request to {request.url.origin} on connection to {self._origin}"
            )
    
        try:
            async with self._request_lock:
                if self._connection is None:
                    stream = await self._connect(request)
    
                    ssl_object = stream.get_extra_info("ssl_object")
                    http2_negotiated = (
                        ssl_object is not None
                        and ssl_object.selected_alpn_protocol() == "h2"
                    )
                    if http2_negotiated or (self._http2 and not self._http1):
                        from .http2 import AsyncHTTP2Connection
    
                        self._connection = AsyncHTTP2Connection(
                            origin=self._origin,
                            stream=stream,
                            keepalive_expiry=self._keepalive_expiry,
                        )
                    else:
                        self._connection = AsyncHTTP11Connection(
                            origin=self._origin,
                            stream=stream,
                            keepalive_expiry=self._keepalive_expiry,
                        )
        except BaseException as exc:
            self._connect_failed = True
>           raise exc

.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncHTTPConnection [CONNECTION FAILED]>, request = <Request [b'POST']>

    async def handle_async_request(self, request: Request) -> Response:
        if not self.can_handle_request(request.url.origin):
            raise RuntimeError(
                f"Attempted to send request to {request.url.origin} on connection to {self._origin}"
            )
    
        try:
            async with self._request_lock:
                if self._connection is None:
>                   stream = await self._connect(request)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncHTTPConnection [CONNECTION FAILED]>, request = <Request [b'POST']>

    async def _connect(self, request: Request) -> AsyncNetworkStream:
        timeouts = request.extensions.get("timeout", {})
        sni_hostname = request.extensions.get("sni_hostname", None)
        timeout = timeouts.get("connect", None)
    
        retries_left = self._retries
        delays = exponential_backoff(factor=RETRIES_BACKOFF_FACTOR)
    
        while True:
            try:
                if self._uds is None:
                    kwargs = {
                        "host": self._origin.host.decode("ascii"),
                        "port": self._origin.port,
                        "local_address": self._local_address,
                        "timeout": timeout,
                        "socket_options": self._socket_options,
                    }
                    async with Trace("connect_tcp", logger, request, kwargs) as trace:
>                       stream = await self._network_backend.connect_tcp(**kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpcore._backends.auto.AutoBackend object at 0x7eb628901220>
host = 'host.docker.internal', port = 11434, timeout = 5.0, local_address = None
socket_options = None

    async def connect_tcp(
        self,
        host: str,
        port: int,
        timeout: float | None = None,
        local_address: str | None = None,
        socket_options: typing.Iterable[SOCKET_OPTION] | None = None,
    ) -> AsyncNetworkStream:
        await self._init_backend()
>       return await self._backend.connect_tcp(
            host,
            port,
            timeout=timeout,
            local_address=local_address,
            socket_options=socket_options,
        )

.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpcore.AnyIOBackend object at 0x7eb5fa93ba70>
host = 'host.docker.internal', port = 11434, timeout = 5.0, local_address = None
socket_options = []

    async def connect_tcp(
        self,
        host: str,
        port: int,
        timeout: float | None = None,
        local_address: str | None = None,
        socket_options: typing.Iterable[SOCKET_OPTION] | None = None,
    ) -> AsyncNetworkStream:  # pragma: nocover
        if socket_options is None:
            socket_options = []
        exc_map = {
            TimeoutError: ConnectTimeout,
            OSError: ConnectError,
            anyio.BrokenResourceError: ConnectError,
        }
>       with map_exceptions(exc_map):

.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb5fa97dfd0>
typ = <class 'socket.gaierror'>
value = gaierror(-2, 'Name or service not known')
traceback = <traceback object at 0x7eb5fa985f80>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

map = {<class 'TimeoutError'>: <class 'httpcore.ConnectTimeout'>, <class 'OSError'>: <class 'httpcore.ConnectError'>, <class 'anyio.BrokenResourceError'>: <class 'httpcore.ConnectError'>}

    @contextlib.contextmanager
    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:
        try:
            yield
        except Exception as exc:  # noqa: PIE786
            for from_exc, to_exc in map.items():
                if isinstance(exc, from_exc):
>                   raise to_exc(exc) from exc
E                   httpcore.ConnectError: [Errno -2] Name or service not known

.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14: ConnectError

The above exception was the direct cause of the following exception:

self = <openai.AsyncOpenAI object at 0x7eb6285ff440>
cast_to = <class 'openai.types.create_embedding_response.CreateEmbeddingResponse'>
options = FinalRequestOptions(method='post', url='/embeddings', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT...s CEO while Daniela Amodei is President.'], 'model': 'nomic-embed-text', 'encoding_format': 'base64'}, extra_json=None)

    async def request(
        self,
        cast_to: Type[ResponseT],
        options: FinalRequestOptions,
        *,
        stream: bool = False,
        stream_cls: type[_AsyncStreamT] | None = None,
    ) -> ResponseT | _AsyncStreamT:
        if self._platform is None:
            # `get_platform` can make blocking IO calls so we
            # execute it earlier while we are in an async context
            self._platform = await asyncify(get_platform)()
    
        cast_to = self._maybe_override_cast_to(cast_to, options)
    
        # create a copy of the options we were given so that if the
        # options are mutated later & we then retry, the retries are
        # given the original options
        input_options = model_copy(options)
        if input_options.idempotency_key is None and input_options.method.lower() != "get":
            # ensure the idempotency key is reused between requests
            input_options.idempotency_key = self._idempotency_key()
    
        response: httpx.Response | None = None
        max_retries = input_options.get_max_retries(self.max_retries)
    
        retries_taken = 0
        for retries_taken in range(max_retries + 1):
            options = model_copy(input_options)
            options = await self._prepare_options(options)
    
            remaining_retries = max_retries - retries_taken
            request = self._build_request(options, retries_taken=retries_taken)
            await self._prepare_request(request)
    
            kwargs: HttpxSendArgs = {}
            if self.custom_auth is not None:
                kwargs["auth"] = self.custom_auth
    
            if options.follow_redirects is not None:
                kwargs["follow_redirects"] = options.follow_redirects
    
            log.debug("Sending HTTP Request: %s %s", request.method, request.url)
    
            response = None
            try:
>               response = await self._client.send(
                    request,
                    stream=stream or self._should_stream_response_body(request=request),
                    **kwargs,
                )

.venv/lib/python3.12/site-packages/openai/_base_client.py:1529: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai._base_client.AsyncHttpxClientWrapper object at 0x7eb6285ff410>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>

    async def send(
        self,
        request: Request,
        *,
        stream: bool = False,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
    ) -> Response:
        """
        Send a request.
    
        The request is sent as-is, unmodified.
    
        Typically you'll want to build one with `AsyncClient.build_request()`
        so that any client-level configuration is merged into the request,
        but passing an explicit `httpx.Request()` is supported as well.
    
        See also: [Request instances][0]
    
        [0]: /advanced/clients/#request-instances
        """
        if self._state == ClientState.CLOSED:
            raise RuntimeError("Cannot send a request, as the client has been closed.")
    
        self._state = ClientState.OPENED
        follow_redirects = (
            self.follow_redirects
            if isinstance(follow_redirects, UseClientDefault)
            else follow_redirects
        )
    
        self._set_timeout(request)
    
        auth = self._build_request_auth(request, auth)
    
>       response = await self._send_handling_auth(
            request,
            auth=auth,
            follow_redirects=follow_redirects,
            history=[],
        )

.venv/lib/python3.12/site-packages/httpx/_client.py:1629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai._base_client.AsyncHttpxClientWrapper object at 0x7eb6285ff410>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>
auth = <httpx.Auth object at 0x7eb5fa97cce0>, follow_redirects = True
history = []

    async def _send_handling_auth(
        self,
        request: Request,
        auth: Auth,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        auth_flow = auth.async_auth_flow(request)
        try:
            request = await auth_flow.__anext__()
    
            while True:
>               response = await self._send_handling_redirects(
                    request,
                    follow_redirects=follow_redirects,
                    history=history,
                )

.venv/lib/python3.12/site-packages/httpx/_client.py:1657: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai._base_client.AsyncHttpxClientWrapper object at 0x7eb6285ff410>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>
follow_redirects = True, history = []

    async def _send_handling_redirects(
        self,
        request: Request,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        while True:
            if len(history) > self.max_redirects:
                raise TooManyRedirects(
                    "Exceeded maximum allowed redirects.", request=request
                )
    
            for hook in self._event_hooks["request"]:
                await hook(request)
    
>           response = await self._send_single_request(request)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_client.py:1694: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai._base_client.AsyncHttpxClientWrapper object at 0x7eb6285ff410>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>

    async def _send_single_request(self, request: Request) -> Response:
        """
        Sends a single request, without handling any redirections.
        """
        transport = self._transport_for_url(request.url)
        start = time.perf_counter()
    
        if not isinstance(request.stream, AsyncByteStream):
            raise RuntimeError(
                "Attempted to send an sync request with an AsyncClient instance."
            )
    
        with request_context(request=request):
>           response = await transport.handle_async_request(request)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_client.py:1730: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncHTTPTransport object at 0x7eb5fbf3ec00>
request = <Request('POST', 'http://host.docker.internal:11434/v1/embeddings')>

    async def handle_async_request(
        self,
        request: Request,
    ) -> Response:
        assert isinstance(request.stream, AsyncByteStream)
        import httpcore
    
        req = httpcore.Request(
            method=request.method,
            url=httpcore.URL(
                scheme=request.url.raw_scheme,
                host=request.url.raw_host,
                port=request.url.port,
                target=request.url.raw_path,
            ),
            headers=request.headers.raw,
            content=request.stream,
            extensions=request.extensions,
        )
>       with map_httpcore_exceptions():

.venv/lib/python3.12/site-packages/httpx/_transports/default.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb5fa97d340>
typ = <class 'httpcore.ConnectError'>
value = ConnectError(gaierror(-2, 'Name or service not known'))
traceback = <traceback object at 0x7eb5fa986a80>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextlib.contextmanager
    def map_httpcore_exceptions() -> typing.Iterator[None]:
        global HTTPCORE_EXC_MAP
        if len(HTTPCORE_EXC_MAP) == 0:
            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()
        try:
            yield
        except Exception as exc:
            mapped_exc = None
    
            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():
                if not isinstance(exc, from_exc):
                    continue
                # We want to map to the most specific exception we can find.
                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to
                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.
                if mapped_exc is None or issubclass(to_exc, mapped_exc):
                    mapped_exc = to_exc
    
            if mapped_exc is None:  # pragma: no cover
                raise
    
            message = str(exc)
>           raise mapped_exc(message) from exc
E           httpx.ConnectError: [Errno -2] Name or service not known

.venv/lib/python3.12/site-packages/httpx/_transports/default.py:118: ConnectError

The above exception was the direct cause of the following exception:

self = <src.core.generation.infrastructure.providers.ollama.OllamaEmbeddingProvider object at 0x7eb6285fdee0>
texts = ['# Integration Test Document', '# Integration Test Document\n\n## Company Information\n\nAnthropic is an AI safety co...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']
model = 'nomic-embed-text', dimensions = 768
kwargs = {'metadata': {'document_id': 'doc_313242de275f5512'}}
start_time = 32478.933257622, work_class = 'ingestion'
limiter = <src.shared.llm_capacity.RedisLLMCapacityLimiter object at 0x7eb6287207a0>

    async def embed(
        self,
        texts: list[str],
        model: str | None = None,
        dimensions: int | None = None,
        **kwargs,
    ) -> EmbeddingResult:
        """Generate embeddings using Ollama (via OpenAI compatible API)."""
        model = model or self.default_model
        start_time = time.perf_counter()
    
        work_class = kwargs.pop("work_class", "ingestion")
        limiter = get_ollama_capacity_limiter()
    
        try:
            try:
                async with limiter.hold(work_class=work_class):
>                   response = await self.client.embeddings.create(
                        model=model,
                        input=texts,
                    )

src/core/generation/infrastructure/providers/ollama.py:447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai.resources.embeddings.AsyncEmbeddings object at 0x7eb628454410>

    async def create(
        self,
        *,
        input: Union[str, SequenceNotStr[str], Iterable[int], Iterable[Iterable[int]]],
        model: Union[str, EmbeddingModel],
        dimensions: int | Omit = omit,
        encoding_format: Literal["float", "base64"] | Omit = omit,
        user: str | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> CreateEmbeddingResponse:
        """
        Creates an embedding vector representing the input text.
    
        Args:
          input: Input text to embed, encoded as a string or array of tokens. To embed multiple
              inputs in a single request, pass an array of strings or array of token arrays.
              The input must not exceed the max input tokens for the model (8192 tokens for
              all embedding models), cannot be an empty string, and any array must be 2048
              dimensions or less.
              [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
              for counting tokens. In addition to the per-input token limit, all embedding
              models enforce a maximum of 300,000 tokens summed across all inputs in a single
              request.
    
          model: ID of the model to use. You can use the
              [List models](https://platform.openai.com/docs/api-reference/models/list) API to
              see all of your available models, or see our
              [Model overview](https://platform.openai.com/docs/models) for descriptions of
              them.
    
          dimensions: The number of dimensions the resulting output embeddings should have. Only
              supported in `text-embedding-3` and later models.
    
          encoding_format: The format to return the embeddings in. Can be either `float` or
              [`base64`](https://pypi.org/project/pybase64/).
    
          user: A unique identifier representing your end-user, which can help OpenAI to monitor
              and detect abuse.
              [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
    
          extra_headers: Send extra headers
    
          extra_query: Add additional query parameters to the request
    
          extra_body: Add additional JSON properties to the request
    
          timeout: Override the client-level default timeout for this request, in seconds
        """
        params = {
            "input": input,
            "model": model,
            "user": user,
            "dimensions": dimensions,
            "encoding_format": encoding_format,
        }
        if not is_given(encoding_format):
            params["encoding_format"] = "base64"
    
        def parser(obj: CreateEmbeddingResponse) -> CreateEmbeddingResponse:
            if is_given(encoding_format):
                # don't modify the response object if a user explicitly asked for a format
                return obj
    
            if not obj.data:
                raise ValueError("No embedding data received")
    
            for embedding in obj.data:
                data = cast(object, embedding.embedding)
                if not isinstance(data, str):
                    continue
                if not has_numpy():
                    # use array for base64 optimisation
                    embedding.embedding = array.array("f", base64.b64decode(data)).tolist()
                else:
                    embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]
                        base64.b64decode(data), dtype="float32"
                    ).tolist()
    
            return obj
    
>       return await self._post(
            "/embeddings",
            body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                post_parser=parser,
            ),
            cast_to=CreateEmbeddingResponse,
        )

.venv/lib/python3.12/site-packages/openai/resources/embeddings.py:251: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai.AsyncOpenAI object at 0x7eb6285ff440>, path = '/embeddings'

    async def post(
        self,
        path: str,
        *,
        cast_to: Type[ResponseT],
        body: Body | None = None,
        files: RequestFiles | None = None,
        options: RequestOptions = {},
        stream: bool = False,
        stream_cls: type[_AsyncStreamT] | None = None,
    ) -> ResponseT | _AsyncStreamT:
        opts = FinalRequestOptions.construct(
            method="post", url=path, json_data=body, files=await async_to_httpx_files(files), **options
        )
>       return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/openai/_base_client.py:1794: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <openai.AsyncOpenAI object at 0x7eb6285ff440>
cast_to = <class 'openai.types.create_embedding_response.CreateEmbeddingResponse'>
options = FinalRequestOptions(method='post', url='/embeddings', params={}, headers=NOT_GIVEN, max_retries=NOT_GIVEN, timeout=NOT...s CEO while Daniela Amodei is President.'], 'model': 'nomic-embed-text', 'encoding_format': 'base64'}, extra_json=None)

    async def request(
        self,
        cast_to: Type[ResponseT],
        options: FinalRequestOptions,
        *,
        stream: bool = False,
        stream_cls: type[_AsyncStreamT] | None = None,
    ) -> ResponseT | _AsyncStreamT:
        if self._platform is None:
            # `get_platform` can make blocking IO calls so we
            # execute it earlier while we are in an async context
            self._platform = await asyncify(get_platform)()
    
        cast_to = self._maybe_override_cast_to(cast_to, options)
    
        # create a copy of the options we were given so that if the
        # options are mutated later & we then retry, the retries are
        # given the original options
        input_options = model_copy(options)
        if input_options.idempotency_key is None and input_options.method.lower() != "get":
            # ensure the idempotency key is reused between requests
            input_options.idempotency_key = self._idempotency_key()
    
        response: httpx.Response | None = None
        max_retries = input_options.get_max_retries(self.max_retries)
    
        retries_taken = 0
        for retries_taken in range(max_retries + 1):
            options = model_copy(input_options)
            options = await self._prepare_options(options)
    
            remaining_retries = max_retries - retries_taken
            request = self._build_request(options, retries_taken=retries_taken)
            await self._prepare_request(request)
    
            kwargs: HttpxSendArgs = {}
            if self.custom_auth is not None:
                kwargs["auth"] = self.custom_auth
    
            if options.follow_redirects is not None:
                kwargs["follow_redirects"] = options.follow_redirects
    
            log.debug("Sending HTTP Request: %s %s", request.method, request.url)
    
            response = None
            try:
                response = await self._client.send(
                    request,
                    stream=stream or self._should_stream_response_body(request=request),
                    **kwargs,
                )
            except httpx.TimeoutException as err:
                log.debug("Encountered httpx.TimeoutException", exc_info=True)
    
                if remaining_retries > 0:
                    await self._sleep_for_retry(
                        retries_taken=retries_taken,
                        max_retries=max_retries,
                        options=input_options,
                        response=None,
                    )
                    continue
    
                log.debug("Raising timeout error")
                raise APITimeoutError(request=request) from err
            except Exception as err:
                log.debug("Encountered Exception", exc_info=True)
    
                if remaining_retries > 0:
                    await self._sleep_for_retry(
                        retries_taken=retries_taken,
                        max_retries=max_retries,
                        options=input_options,
                        response=None,
                    )
                    continue
    
                log.debug("Raising connection error")
>               raise APIConnectionError(request=request) from err
E               openai.APIConnectionError: Connection error.

.venv/lib/python3.12/site-packages/openai/_base_client.py:1561: APIConnectionError

During handling of the above exception, another exception occurred:

self = <AsyncRetrying object at 0x7eb6285fe1b0 (stop=<tenacity.stop.stop_after_attempt object at 0x7eb6613be750>, wait=<tenac...0x7eb6613be180>, before=<function before_nothing at 0x7eb6620b9f80>, after=<function after_nothing at 0x7eb6613ad300>)>
fn = <function EmbeddingService._embed_batch_with_retry at 0x7eb6613c1f80>
args = (<src.core.retrieval.application.embeddings_service.EmbeddingService object at 0x7eb6285fdfa0>,)
kwargs = {'dimensions': 768, 'metadata': {'document_id': 'doc_313242de275f5512'}, 'model': 'nomic-embed-text', 'texts': ['# Int...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']}
retry_state = <RetryCallState 139320826520416: attempt #5; slept for 15.0; last result: failed (ProviderUnavailableError [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.)>
do = <tenacity.DoAttempt object at 0x7eb5fa97f7d0>

    async def __call__(  # type: ignore[override]
        self, fn: WrappedFn, *args: t.Any, **kwargs: t.Any
    ) -> WrappedFnReturnT:
        self.begin()
    
        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
            do = await self.iter(retry_state=retry_state)
            if isinstance(do, DoAttempt):
                try:
>                   result = await fn(*args, **kwargs)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.retrieval.application.embeddings_service.EmbeddingService object at 0x7eb6285fdfa0>
texts = ['# Integration Test Document', '# Integration Test Document\n\n## Company Information\n\nAnthropic is an AI safety co...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']
model = 'nomic-embed-text', dimensions = 768
metadata = {'document_id': 'doc_313242de275f5512'}

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        retry=retry_if_exception_type((RateLimitError, ProviderUnavailableError)),
        before_sleep=lambda retry_state: logger.warning(
            f"Retrying embedding after {retry_state.outcome.exception()}"
        ),
    )
    async def _embed_batch_with_retry(
        self,
        texts: list[str],
        model: str,
        dimensions: int | None,
        metadata: dict[str, Any] | None = None,
    ) -> EmbeddingResult:
        """Embed a batch with automatic retries."""
>       return await self.provider.embed(
            texts=texts,
            model=model,
            dimensions=dimensions,
            metadata=metadata,
        )

src/core/retrieval/application/embeddings_service.py:231: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.generation.infrastructure.providers.ollama.OllamaEmbeddingProvider object at 0x7eb6285fdee0>
texts = ['# Integration Test Document', '# Integration Test Document\n\n## Company Information\n\nAnthropic is an AI safety co...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']
model = 'nomic-embed-text', dimensions = 768
kwargs = {'metadata': {'document_id': 'doc_313242de275f5512'}}
start_time = 32478.933257622, work_class = 'ingestion'
limiter = <src.shared.llm_capacity.RedisLLMCapacityLimiter object at 0x7eb6287207a0>

    async def embed(
        self,
        texts: list[str],
        model: str | None = None,
        dimensions: int | None = None,
        **kwargs,
    ) -> EmbeddingResult:
        """Generate embeddings using Ollama (via OpenAI compatible API)."""
        model = model or self.default_model
        start_time = time.perf_counter()
    
        work_class = kwargs.pop("work_class", "ingestion")
        limiter = get_ollama_capacity_limiter()
    
        try:
            try:
                async with limiter.hold(work_class=work_class):
                    response = await self.client.embeddings.create(
                        model=model,
                        input=texts,
                    )
            except TimeoutError as e:
                raise RateLimitError(
                    str(e),
                    provider=self.provider_name,
                    model=model,
                    retry_after=1.0,
                )
    
            elapsed_ms = (time.perf_counter() - start_time) * 1000
    
            # Extract embeddings in order
            embeddings = [item.embedding for item in sorted(response.data, key=lambda x: x.index)]
    
            # Get dimensions from first embedding
            actual_dimensions = len(embeddings[0]) if embeddings else 0
    
            usage = TokenUsage(
                input_tokens=response.usage.total_tokens if response.usage else 0,
                output_tokens=0,
            )
    
            result = EmbeddingResult(
                embeddings=embeddings,
                model=model,
                provider=self.provider_name,
                usage=usage,
                dimensions=actual_dimensions,
                latency_ms=elapsed_ms,
                cost_estimate=0.0,  # Local is free
                metadata={"response_id": None},  # Ollama embed response might not have ID
            )
    
            # Record usage if tracker is available
            if self.config.usage_tracker:
                span_context = trace.get_current_span().get_span_context()
                trace_id = (
                    format(span_context.trace_id, "032x") if span_context.is_valid else None
                )
    
                # Merge metadata from kwargs (e.g. document_id) with result metadata
                usage_metadata = {**result.metadata, **(kwargs.get("metadata") or {})}
    
                await self.config.usage_tracker.record_usage(
                    tenant_id=get_current_tenant() or "default",
                    operation="embedding",
                    provider=self.provider_name,
                    model=model,
                    usage=usage,
                    cost=result.cost_estimate,
                    request_id=get_request_id(),
                    trace_id=trace_id,
                    metadata=usage_metadata,
                )
    
            return result
    
        except RateLimitError:
            raise
        except Exception as e:
>           self._handle_error(e, model)

src/core/generation/infrastructure/providers/ollama.py:510: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.generation.infrastructure.providers.ollama.OllamaEmbeddingProvider object at 0x7eb6285fdee0>
e = APIConnectionError('Connection error.'), model = 'nomic-embed-text'

    def _handle_error(self, e: Exception, model: str) -> None:
        """Convert OpenAI exceptions to provider exceptions."""
        error_type = type(e).__name__
    
        if "RateLimitError" in error_type:
            raise RateLimitError(
                str(e),
                provider=self.provider_name,
                model=model,
                retry_after=60.0,
            )
        elif "AuthenticationError" in error_type:
            raise AuthenticationError(
                str(e),
                provider=self.provider_name,
                model=model,
            )
        elif "APIConnectionError" in error_type or "Timeout" in error_type:
>           raise ProviderUnavailableError(
                f"Cannot connect to Ollama at {self.config.base_url}: {e}",
                provider=self.provider_name,
                model=model,
            )
E           src.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.

src/core/generation/infrastructure/providers/ollama.py:531: ProviderUnavailableError

The above exception was the direct cause of the following exception:

self = <@task: src.workers.tasks.process_document of graphrag at 0x7eb66151a960>
document_id = 'doc_313242de275f5512', tenant_id = 'integration_test_tenant'

    @celery_app.task(
        bind=True,
        name="src.workers.tasks.process_document",
        base=BaseTask,
        max_retries=3,
        default_retry_delay=60,
        queue="high_priority",
    )
    def process_document(self, document_id: str, tenant_id: str) -> dict:
        """
        Process a document through the full ingestion pipeline.
    
        Steps:
        1. Fetch document from DB
        2. Update status to EXTRACTING
        3. Run extraction (FallbackManager)
        4. Update status to CLASSIFYING
        5. Run domain classification
        6. Update status to CHUNKING
        7. Run semantic chunking
        8. Update status to READY
    
        Args:
            document_id: ID of the document to process.
            tenant_id: Tenant for context.
    
        Returns:
            dict: Processing result summary.
        """
        logger.info(f"[Task {self.request.id}] Starting processing for document {document_id}")
    
        try:
>           result = run_async(_process_document_async(document_id, tenant_id, self.request.id))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

src/workers/tasks.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

coro = <coroutine object _process_document_async at 0x43470fe0>

    def run_async(coro):
        """Helper to run async code in sync Celery task."""
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None
    
        if loop and loop.is_running():
            # Prevent "Cannot run the event loop while another loop is running"
            # by offloading the async execution to a separate thread with its own loop.
            logger.debug(
                "Detected active event loop %s, offloading coroutine execution to a worker thread",
                loop,
            )
            import concurrent.futures
    
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
    
                def runner():
                    logger.debug("Worker thread started for asyncio.run")
                    try:
                        res = asyncio.run(coro)
                        logger.debug("Worker thread completed asyncio.run")
                        return res
                    except Exception as e:
                        logger.debug("Worker thread failed while running coroutine: %s", e)
                        raise
    
                future = executor.submit(runner)
                logger.debug("Waiting for worker thread result")
>               res = future.result()
                      ^^^^^^^^^^^^^^^

src/workers/tasks.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = None, timeout = None

    def result(self, timeout=None):
        """Return the result of the call that the future represents.
    
        Args:
            timeout: The number of seconds to wait for the result if the future
                isn't done. If None, then there is no limit on the wait time.
    
        Returns:
            The result of the call that the future represents.
    
        Raises:
            CancelledError: If the future was cancelled.
            TimeoutError: If the future didn't finish executing before the given
                timeout.
            Exception: If the call raised then that exception will be raised.
        """
        try:
            with self._condition:
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
                    return self.__get_result()
    
                self._condition.wait(timeout)
    
                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:
                    raise CancelledError()
                elif self._state == FINISHED:
>                   return self.__get_result()
                           ^^^^^^^^^^^^^^^^^^^

/usr/lib/python3.12/concurrent/futures/_base.py:456: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = None

    def __get_result(self):
        if self._exception:
            try:
>               raise self._exception

/usr/lib/python3.12/concurrent/futures/_base.py:401: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = None

    def run(self):
        if not self.future.set_running_or_notify_cancel():
            return
    
        try:
>           result = self.fn(*self.args, **self.kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/usr/lib/python3.12/concurrent/futures/thread.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def runner():
        logger.debug("Worker thread started for asyncio.run")
        try:
>           res = asyncio.run(coro)
                  ^^^^^^^^^^^^^^^^^

src/workers/tasks.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

main = <coroutine object _process_document_async at 0x43470fe0>

    def run(main, *, debug=None, loop_factory=None):
        """Execute the coroutine and return the result.
    
        This function runs the passed coroutine, taking care of
        managing the asyncio event loop, finalizing asynchronous
        generators and closing the default executor.
    
        This function cannot be called when another asyncio event loop is
        running in the same thread.
    
        If debug is True, the event loop will be run in debug mode.
    
        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.
    
        The executor is given a timeout duration of 5 minutes to shutdown.
        If the executor hasn't finished within that duration, a warning is
        emitted and the executor is closed.
    
        Example:
    
            async def main():
                await asyncio.sleep(1)
                print('hello')
    
            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            # fail fast with short traceback
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")
    
        with Runner(debug=debug, loop_factory=loop_factory) as runner:
>           return runner.run(main)
                   ^^^^^^^^^^^^^^^^

/usr/lib/python3.12/asyncio/runners.py:194: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <asyncio.runners.Runner object at 0x7eb62853e030>
coro = <coroutine object _process_document_async at 0x43470fe0>

    def run(self, coro, *, context=None):
        """Run a coroutine inside the embedded event loop."""
        if not coroutines.iscoroutine(coro):
            raise ValueError("a coroutine was expected, got {!r}".format(coro))
    
        if events._get_running_loop() is not None:
            # fail fast with short traceback
            raise RuntimeError(
                "Runner.run() cannot be called from a running event loop")
    
        self._lazy_init()
    
        if context is None:
            context = self._context
        task = self._loop.create_task(coro, context=context)
    
        if (threading.current_thread() is threading.main_thread()
            and signal.getsignal(signal.SIGINT) is signal.default_int_handler
        ):
            sigint_handler = functools.partial(self._on_sigint, main_task=task)
            try:
                signal.signal(signal.SIGINT, sigint_handler)
            except ValueError:
                # `signal.signal` may throw if `threading.main_thread` does
                # not support signals (e.g. embedded interpreter with signals
                # not registered - see gh-91880)
                sigint_handler = None
        else:
            sigint_handler = None
    
        self._interrupt_count = 0
        try:
>           return self._loop.run_until_complete(task)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

/usr/lib/python3.12/asyncio/runners.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=True debug=False>
future = <Task finished name='Task-88' coro=<_process_document_async() done, defined at /home/daniele/Amber_2.0/src/workers/tasks.py:416> exception=RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)>

    def run_until_complete(self, future):
        """Run until the Future is done.
    
        If the argument is a coroutine, it is wrapped in a Task.
    
        WARNING: It would be disastrous to call run_until_complete()
        with the same coroutine twice -- it would wrap it in two
        different Tasks and that can't be good.
    
        Return the Future's result, or raise its exception.
        """
        self._check_closed()
        self._check_running()
    
        new_task = not futures.isfuture(future)
        future = tasks.ensure_future(future, loop=self)
        if new_task:
            # An exception is raised if the future didn't complete, so there
            # is no need to log the "destroy pending task" message
            future._log_destroy_pending = False
    
        future.add_done_callback(_run_until_complete_cb)
        try:
            self.run_forever()
        except:
            if new_task and future.done() and not future.cancelled():
                # The coroutine raised a BaseException. Consume the exception
                # to not log a warning, the caller doesn't have access to the
                # local task.
                future.exception()
            raise
        finally:
            future.remove_done_callback(_run_until_complete_cb)
        if not future.done():
            raise RuntimeError('Event loop stopped before Future completed.')
    
>       return future.result()
               ^^^^^^^^^^^^^^^

/usr/lib/python3.12/asyncio/base_events.py:687: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

document_id = 'doc_313242de275f5512', tenant_id = 'integration_test_tenant'
task_id = '8c2b225b-9e41-4807-beb2-9b75309904d8'

    async def _process_document_async(document_id: str, tenant_id: str, task_id: str) -> dict:
        """
        Async implementation of document processing.
        """
        from sqlalchemy import select
        from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
        from sqlalchemy.orm import sessionmaker
    
        from src.amber_platform.composition_root import build_vector_store_factory, platform
        from src.api.config import settings
    
        # Context isolation: Reset EVERYTHING that might be stale or bound to a closed loop
        deep_reset_singletons()
    
        from src.amber_platform.composition_root import configure_settings
    
        configure_settings(settings)
    
        from src.core.database.session import configure_database
    
        configure_database(settings.db.database_url)
    
        from src.shared.kernel.runtime import configure_settings as configure_runtime_settings
    
        configure_runtime_settings(settings)
    
        # Move DB initialization earlier to fetch tenant config
        # ERROR FIX: Ensure domain models are imported before session usage to avoid Mapper errors
        from src.core.ingestion.domain.chunk import Chunk  # noqa: F401
        from src.core.ingestion.domain.document import Document  # noqa: F401
    
        engine = create_async_engine(settings.db.database_url)
        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
    
        # Fetch Tenant Config for correct Provider Init
        from src.core.tenants.infrastructure.repositories.postgres_tenant_repository import (
            PostgresTenantRepository,
        )
    
        resolved_ollama_url = settings.ollama_base_url
        tenant_runtime_config: dict = {}
        try:
            async with async_session() as tmp_session:
                t_repo = PostgresTenantRepository(tmp_session)
                t_obj = await t_repo.get(tenant_id)
                if t_obj and t_obj.config:
                    tenant_runtime_config = t_obj.config
                    resolved_ollama_url = t_obj.config.get("ollama_base_url") or resolved_ollama_url
        except Exception as e:
            logger.warning(f"Failed to fetch tenant config for provider init: {e}")
    
        from src.core.generation.infrastructure.providers.factory import init_providers
    
        init_providers(
            openai_api_key=settings.openai_api_key,
            anthropic_api_key=settings.anthropic_api_key,
            default_llm_provider=settings.default_llm_provider,
            default_llm_model=settings.default_llm_model,
            default_embedding_provider=settings.default_embedding_provider,
            default_embedding_model=settings.default_embedding_model,
            ollama_base_url=resolved_ollama_url,
            llm_fallback_local=settings.llm_fallback_local,
            llm_fallback_economy=settings.llm_fallback_economy,
            llm_fallback_standard=settings.llm_fallback_standard,
            llm_fallback_premium=settings.llm_fallback_premium,
            embedding_fallback_order=settings.embedding_fallback_order,
        )
    
        from src.core.graph.domain.ports.graph_client import set_graph_client
        from src.core.graph.domain.ports.graph_extractor import set_graph_extractor
        from src.core.graph.application.sync_config import resolve_graph_sync_runtime_config
        from src.core.ingestion.infrastructure.extraction.graph_extractor import GraphExtractor
    
        graph_sync_config = resolve_graph_sync_runtime_config(
            settings=settings,
            tenant_config=tenant_runtime_config,
        )
        set_graph_extractor(
            GraphExtractor(
                use_gleaning=graph_sync_config.use_gleaning,
                max_gleaning_steps=graph_sync_config.max_gleaning_steps,
            )
        )
        set_graph_client(platform.neo4j_client)
    
        try:
            async with async_session() as session:
                # Initialize services
                from src.core.events.dispatcher import EventDispatcher
                from src.core.ingestion.application.ingestion_service import IngestionService
                from src.core.ingestion.infrastructure.repositories.postgres_document_repository import (
                    PostgresDocumentRepository,
                )
                from src.core.ingestion.infrastructure.uow.postgres_uow import PostgresUnitOfWork
                from src.infrastructure.adapters.redis_state_publisher import RedisStatePublisher
    
                vector_store_factory = build_vector_store_factory()
                event_dispatcher = EventDispatcher(RedisStatePublisher())
    
                repo = PostgresDocumentRepository(session)
                tenant_repo = PostgresTenantRepository(session)
                uow = PostgresUnitOfWork(session)
    
                # Validation
                document = await repo.get(document_id)
                if not document:
                    raise ValueError(f"Document {document_id} not found")
    
                service = IngestionService(
                    document_repository=repo,
                    tenant_repository=tenant_repo,
                    unit_of_work=uow,
                    storage_client=platform.minio_client,
                    neo4j_client=platform.neo4j_client,
                    vector_store=None,  # vector_store_factory used internally or passed if needed?
                    # In previous code vector_store was None but vector_store_factory passed.
                    settings=settings,
                    event_dispatcher=event_dispatcher,
                    vector_store_factory=vector_store_factory,
                )
    
                # Publish starting event
                _publish_status(document_id, DocumentStatus.EXTRACTING.value, 10)
    
                # Process document (this does extraction, classification, chunking)
>               await service.process_document(document_id)

src/workers/tasks.py:541: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.ingestion.application.ingestion_service.IngestionService object at 0x7eb62856a9c0>
document_id = 'doc_313242de275f5512'

    async def process_document(self, document_id: str):
        """
        Orchestrate the document ingestion pipeline.
        """
        logger.debug("Starting process_document for %s", document_id)
    
        start_time = time.time()
    
        # 1. Fetch Document
        document = await self.document_repository.get(document_id)
    
        if not document:
            raise ValueError(f"Document {document_id} not found")
    
        # Set tenant context for this background task
        set_current_tenant(document.tenant_id)
    
        # 2. Check State & Transition (INGESTED -> EXTRACTING)
        updated = await self.document_repository.update_status(
            document_id, DocumentStatus.EXTRACTING, old_status=DocumentStatus.INGESTED
        )
    
        if not updated:
            # Re-fetch to see why
            document = await self.document_repository.get(document_id)
            logger.warning(
                f"Skipping processing for {document_id}: "
                f"Status is {document.status} (expected INGESTED)"
            )
            return
    
        # Commit directly to release lock/visible state
        await self.unit_of_work.commit()
    
        # Refresh local object to match DB
        document = await self.document_repository.get(document_id)
    
        tenant_config: dict[str, Any] = {}
        if self.tenant_repository:
            try:
                tenant_obj = await self.tenant_repository.get(document.tenant_id)
                if tenant_obj and tenant_obj.config:
                    tenant_config = tenant_obj.config
            except Exception as e:
                logger.warning(f"Failed to load tenant config for ingestion: {e}")
    
        try:
            # 3. Get File from Storage
            # MinIO get_file returns bytes (handled inside wrapper)
            file_content = self.storage.get_file(document.storage_path)
    
            # 4. Extract Content (Fallback Chain)
            import mimetypes
    
            mime_type, _ = mimetypes.guess_type(document.filename)
            if not mime_type:
                mime_type = "application/octet-stream"
    
            extractor = self.content_extractor or get_content_extractor()
            extraction_result = await extractor.extract(
                file_content=file_content, mime_type=mime_type, filename=document.filename
            )
    
            # 5. Classify Domain (Stage 1.4)
            await self.document_repository.update_status(document.id, DocumentStatus.CLASSIFYING)
            await self.unit_of_work.commit()
            document.status = DocumentStatus.CLASSIFYING
    
            await self.event_dispatcher.emit_state_change(
                StateChangeEvent(
                    document_id=document.id,
                    old_status=DocumentStatus.EXTRACTING,
                    new_status=DocumentStatus.CLASSIFYING,
                    tenant_id=document.tenant_id,
                    details={"progress": 20},
                )
            )
    
            from src.core.generation.application.intelligence.classifier import DomainClassifier
            from src.core.generation.application.intelligence.strategies import get_strategy
    
            classifier = DomainClassifier()
            domain = await classifier.classify(extraction_result.content)
            await classifier.close()
    
            # 6. Select Strategy
            strategy = get_strategy(domain.value)
            logger.info(
                f"Classified document {document_id} as {domain.value}. Strategy: {strategy.name}"
            )
    
            document.domain = domain.value
    
            # Metadata: Initial population (Clean Schema)
            # We preserve internal technical fields (content_type, mime_type) for system use
            # but present a cleaner view for the user.
    
            file_ext = document.filename.split(".")[-1] if "." in document.filename else ""
            fmt = "PDF" if file_ext.lower() == "pdf" else file_ext.upper()
    
            # Format creation date DD/MM/YYYY
            # Convert to local time (CET) for user friendliness
            local_dt = document.created_at.astimezone()
            created_date = local_dt.strftime("%d/%m/%Y")
            upload_time = local_dt.strftime("%H:%M")
    
            document.metadata_ = {
                "title": document.filename.rsplit(".", 1)[0],
                "format": fmt,
                "pageCount": extraction_result.metadata.get("page_count")
                if extraction_result.metadata
                else None,
                "creationDate": created_date,
                "uploadTime": upload_time,
                # Technical preservation
                "content_type": mime_type,
                "mime_type": mime_type,
                "file_size": len(file_content),
            }
    
            # 7. Chunk Content using SemanticChunker (Stage 1.5)
            await self.document_repository.update_status(document.id, DocumentStatus.CHUNKING)
            await self.unit_of_work.commit()
            document.status = DocumentStatus.CHUNKING
    
            await self.event_dispatcher.emit_state_change(
                StateChangeEvent(
                    document_id=document.id,
                    old_status=DocumentStatus.CLASSIFYING,
                    new_status=DocumentStatus.CHUNKING,
                    tenant_id=document.tenant_id,
                    details={"progress": 40},
                )
            )
    
            from src.core.ingestion.application.chunking.semantic import SemanticChunker
            from src.core.ingestion.domain.chunk import Chunk, EmbeddingStatus
            from src.shared.identifiers import generate_chunk_id
    
            chunker = SemanticChunker(strategy)
            chunk_data_list = chunker.chunk(
                extraction_result.content,
                document_title=document.filename,
                metadata=extraction_result.metadata,
            )
    
            logger.info(f"Document {document_id} split into {len(chunk_data_list)} chunks")
    
            chunks_to_process = []
            for cd in chunk_data_list:
                chunk = Chunk(
                    id=generate_chunk_id(document.id, cd.index),
                    tenant_id=document.tenant_id,
                    document_id=document.id,
                    index=cd.index,
                    content=cd.content,
                    tokens=cd.token_count,
                    metadata_={
                        "extractor": extraction_result.extractor_used,
                        "confidence": extraction_result.confidence,
                        "extraction_time": extraction_result.extraction_time_ms,
                        "domain": domain.value,
                        "start_char": cd.start_char,
                        "end_char": cd.end_char,
                        **cd.metadata,
                        **extraction_result.metadata,
                    },
                    embedding_status=EmbeddingStatus.PENDING,
                )
                chunks_to_process.append(chunk)
    
            document.chunks = chunks_to_process
            await self.document_repository.save(document)
    
            # 8. Generate Embeddings and Store in Milvus
            await self.document_repository.update_status(document.id, DocumentStatus.EMBEDDING)
            await self.unit_of_work.commit()
            document.status = DocumentStatus.EMBEDDING
    
            await self.event_dispatcher.emit_state_change(
                StateChangeEvent(
                    document_id=document.id,
                    old_status=DocumentStatus.CHUNKING,
                    new_status=DocumentStatus.EMBEDDING,
                    tenant_id=document.tenant_id,
                    details={"progress": 60, "chunk_count": len(chunks_to_process)},
                )
            )
    
            vector_store = None
            try:
                settings = self.settings
                from src.core.generation.domain.ports.provider_factory import (
                    build_provider_factory,
                    get_provider_factory,
                )
                from src.core.retrieval.application.embeddings_service import EmbeddingService
                from src.core.retrieval.application.sparse_embeddings_service import (
                    SparseEmbeddingService,
                )
    
                tenant_obj = await self.tenant_repository.get(document.tenant_id)
                t_config = tenant_obj.config if tenant_obj and tenant_obj.config else {}
    
                sys_prov = settings.default_embedding_provider
                sys_model = settings.default_embedding_model
                sys_dims = settings.embedding_dimensions or 1536
    
                res_prov = t_config.get("embedding_provider") or sys_prov
                res_model = t_config.get("embedding_model") or sys_model
                res_dims = t_config.get("embedding_dimensions") or sys_dims
    
                # Resolve Ollama URL from Tenant Config -> Settings
                res_ollama_url = t_config.get("ollama_base_url") or settings.ollama_base_url
    
                try:
                    factory = build_provider_factory(
                        openai_api_key=settings.openai_api_key,
                        ollama_base_url=res_ollama_url,
                        default_embedding_provider=res_prov,
                        default_embedding_model=res_model,
                    )
                except RuntimeError:
                    factory = get_provider_factory()
    
                # Reduce batch size for Ollama to prevent runner crashes on large inputs
                max_tokens = 2048 if res_prov == "ollama" else None
    
                embedding_service = EmbeddingService(
                    provider=factory.get_embedding_provider(
                        provider_name=res_prov,
                        model=res_model,
                    ),
                    model=res_model,
                    dimensions=res_dims,
                    max_tokens_per_batch=max_tokens,
                )
    
                sparse_service = SparseEmbeddingService()
    
                active_collection = resolve_active_vector_collection(document.tenant_id, t_config)
    
                if self.vector_store_factory:
                    vector_store = self.vector_store_factory(
                        res_dims, collection_name=active_collection
                    )
                else:
                    logger.debug("Using provided vector store")
                    vector_store = self.vector_store
    
                logger.info(
                    f"RESOLVED EMBEDDING CONFIG | Document: {document.id} | Tenant: {document.tenant_id}"
                )
                logger.info(
                    f"  - Tenant Config Provider: {t_config.get('embedding_provider')} (sys default: {sys_prov})"
                )
                logger.info(
                    f"  - Tenant Config Model: {t_config.get('embedding_model')} (sys default: {sys_model})"
                )
                logger.info(f"  - Resolved Provider: {res_prov}")
                logger.info(f"  - Resolved Model: {res_model}")
                logger.info(f"  - Factory: {factory.__class__.__name__}")
    
                # Capture Embedding Metadata
                # Re-assign dict to trigger SQLAlchemy JSONB change tracking
                meta_update = document.metadata_ or {}
                meta_update["embeddingModel"] = f"{res_prov} {res_model}"
                meta_update["vectorStore"] = active_collection
                document.metadata_ = dict(meta_update)
    
                if vector_store is None:
                    raise RuntimeError("Vector store not configured")
    
                chunk_contents = [c.content for c in chunks_to_process]
                logger.debug('Calling embed_texts chunks=%d model=%s', len(chunk_contents), res_model)
    
                # Callback for granular progress (60->70%)
                async def _on_embedding_progress(completed: int, total: int):
                    if total == 0: return
                    # Scale 60 -> 70
                    progress = 60 + int((completed / total) * 10)
                    await self.event_dispatcher.emit_state_change(
                        StateChangeEvent(
                            document_id=document.id,
                            old_status=DocumentStatus.EMBEDDING,
                            new_status=DocumentStatus.EMBEDDING,
                            tenant_id=document.tenant_id,
                            details={"progress": progress, "chunks_completed": completed, "total_chunks": total},
                        )
                    )
    
>               embeddings, stats = await embedding_service.embed_texts(
                    chunk_contents,
                    metadata={"document_id": document.id},
                    progress_callback=_on_embedding_progress
                )

src/core/ingestion/application/ingestion_service.py:465: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.retrieval.application.embeddings_service.EmbeddingService object at 0x7eb6285fdfa0>
texts = ['# Integration Test Document', '# Integration Test Document\n\n## Company Information\n\nAnthropic is an AI safety co...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']
model = 'nomic-embed-text', dimensions = 768, show_progress = False
metadata = {'document_id': 'doc_313242de275f5512'}
progress_callback = <function IngestionService.process_document.<locals>._on_embedding_progress at 0x7eb628587a60>

    async def embed_texts(
        self,
        texts: list[str],
        model: str | None = None,
        dimensions: int | None = None,
        show_progress: bool = False,
        metadata: dict[str, Any] | None = None,
        progress_callback: Callable[[int, int], Awaitable[None]] | None = None,
    ) -> tuple[list[list[float]], EmbeddingStats]:
        """
        Generate embeddings for a list of texts.
    
        Args:
            texts: Texts to embed
            model: Override default model
            dimensions: Override default dimensions
            show_progress: Log progress updates
            progress_callback: Async callback(completed_items, total_items)
    
        Returns:
            Tuple of (embeddings, stats)
            Embeddings are in same order as input texts
        """
        if not texts:
            return [], EmbeddingStats()
    
        model = model or self.model
        dimensions = dimensions or self.dimensions
    
        # Batch the texts
        batches = batch_texts_for_embedding(
            texts=texts,
            model=model,
            max_tokens=self.max_tokens,
            max_items=self.max_items,
        )
    
        stats = EmbeddingStats(
            total_texts=len(texts),
            total_batches=len(batches),
        )
    
        completed_count = 0
        total_count = len(texts)
    
        # Pre-allocate result array
        embeddings: list[list[float] | None] = [None] * len(texts)
    
        # Process batches
        for batch_idx, batch in enumerate(batches):
            if show_progress:
                logger.info(f"Processing batch {batch_idx + 1}/{len(batches)}")
    
            # Extract texts for this batch
            batch_texts = [text for _, text in batch]
    
            # Embed with retries
            # If this fails, we want it to raise an exception so the document fails
            # with a meaningful error (e.g. AuthError) instead of a confusing Milvus error later.
>           result = await self._embed_batch_with_retry(
                texts=batch_texts,
                model=model,
                dimensions=dimensions,
                metadata=metadata,
            )

src/core/retrieval/application/embeddings_service.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<src.core.retrieval.application.embeddings_service.EmbeddingService object at 0x7eb6285fdfa0>,)
kwargs = {'dimensions': 768, 'metadata': {'document_id': 'doc_313242de275f5512'}, 'model': 'nomic-embed-text', 'texts': ['# Int...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']}
copy = <AsyncRetrying object at 0x7eb6285fe1b0 (stop=<tenacity.stop.stop_after_attempt object at 0x7eb6613be750>, wait=<tenac...0x7eb6613be180>, before=<function before_nothing at 0x7eb6620b9f80>, after=<function after_nothing at 0x7eb6613ad300>)>

    @functools.wraps(
        fn, functools.WRAPPER_ASSIGNMENTS + ("__defaults__", "__kwdefaults__")
    )
    async def async_wrapped(*args: t.Any, **kwargs: t.Any) -> t.Any:
        # Always create a copy to prevent overwriting the local contexts when
        # calling the same wrapped functions multiple times in the same stack
        copy = self.copy()
        async_wrapped.statistics = copy.statistics  # type: ignore[attr-defined]
>       return await copy(fn, *args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncRetrying object at 0x7eb6285fe1b0 (stop=<tenacity.stop.stop_after_attempt object at 0x7eb6613be750>, wait=<tenac...0x7eb6613be180>, before=<function before_nothing at 0x7eb6620b9f80>, after=<function after_nothing at 0x7eb6613ad300>)>
fn = <function EmbeddingService._embed_batch_with_retry at 0x7eb6613c1f80>
args = (<src.core.retrieval.application.embeddings_service.EmbeddingService object at 0x7eb6285fdfa0>,)
kwargs = {'dimensions': 768, 'metadata': {'document_id': 'doc_313242de275f5512'}, 'model': 'nomic-embed-text', 'texts': ['# Int...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n## Leadership\n\n\nDario Amodei serves as CEO while Daniela Amodei is President.']}
retry_state = <RetryCallState 139320826520416: attempt #5; slept for 15.0; last result: failed (ProviderUnavailableError [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.)>
do = <tenacity.DoAttempt object at 0x7eb5fa97f7d0>

    async def __call__(  # type: ignore[override]
        self, fn: WrappedFn, *args: t.Any, **kwargs: t.Any
    ) -> WrappedFnReturnT:
        self.begin()
    
        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        while True:
>           do = await self.iter(retry_state=retry_state)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <AsyncRetrying object at 0x7eb6285fe1b0 (stop=<tenacity.stop.stop_after_attempt object at 0x7eb6613be750>, wait=<tenac...0x7eb6613be180>, before=<function before_nothing at 0x7eb6620b9f80>, after=<function after_nothing at 0x7eb6613ad300>)>
retry_state = <RetryCallState 139320826520416: attempt #5; slept for 15.0; last result: failed (ProviderUnavailableError [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.)>

    async def iter(
        self, retry_state: "RetryCallState"
    ) -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa: A003
        self._begin_iter(retry_state)
        result = None
        for action in self.iter_state.actions:
>           result = await action(retry_state)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<RetryCallState 139320826520416: attempt #5; slept for 15.0; last result: failed (ProviderUnavailableError [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.)>,)
kwargs = {}

    async def inner(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:
>       return call(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/tenacity/_utils.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

rs = <RetryCallState 139320826520416: attempt #5; slept for 15.0; last result: failed (ProviderUnavailableError [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.)>

    def exc_check(rs: "RetryCallState") -> None:
        fut = t.cast(Future, rs.outcome)
        retry_exc = self.retry_error_cls(fut)
        if self.reraise:
            raise retry_exc.reraise()
>       raise retry_exc from fut.exception()
E       tenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]

.venv/lib/python3.12/site-packages/tenacity/__init__.py:421: RetryError

During handling of the above exception, another exception occurred:

self = <src.infrastructure.adapters.celery_dispatcher.CeleryTaskDispatcher object at 0x7eb62894f440>
task_name = 'src.workers.tasks.process_document'
args = ['doc_313242de275f5512', 'integration_test_tenant'], kwargs = {}

    async def dispatch(
        self, task_name: str, args: list[Any] | None = None, kwargs: dict[str, Any] | None = None
    ) -> str:
        args = args or []
        kwargs = kwargs or {}
    
        try:
            # We assume task_name matches the registered Celery task name string
            # OR we can map specific domain names to celery function names here if we want strict decoupling.
            # For simplicity, we assume strict naming.
    
            # Note: Celery send_task is synchronous unless we use its async API (rarely used).
            # Usually send_task is fast enough (push to Redis).
            # Check for eager execution
            if celery_app.conf.task_always_eager:
                if task_name in celery_app.tasks:
                    task = celery_app.tasks[task_name]
>                   result = task.apply(args=args, kwargs=kwargs)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

src/infrastructure/adapters/celery_dispatcher.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <@task: src.workers.tasks.process_document of graphrag at 0x7eb66151a960>
args = ['doc_313242de275f5512', 'integration_test_tenant'], kwargs = {}
link = None, link_error = None, task_id = '8c2b225b-9e41-4807-beb2-9b75309904d8'
retries = 0, throw = True, logfile = None, loglevel = None, headers = None
options = {}, build_tracer = <function build_tracer at 0x7eb62854a660>
app = <Celery graphrag at 0x7eb66151a960>, parent_task = None, parent_id = None
root_id = '8c2b225b-9e41-4807-beb2-9b75309904d8'
task = <@task: src.workers.tasks.process_document of graphrag at 0x7eb66151a960>
request = {'callbacks': None, 'delivery_info': {'exchange': None, 'is_eager': True, 'priority': None, 'routing_key': None}, 'errbacks': None, 'headers': None, ...}

    def apply(self, args=None, kwargs=None,
              link=None, link_error=None,
              task_id=None, retries=None, throw=None,
              logfile=None, loglevel=None, headers=None, **options):
        """Execute this task locally, by blocking until the task returns.
    
        Arguments:
            args (Tuple): positional arguments passed on to the task.
            kwargs (Dict): keyword arguments passed on to the task.
            throw (bool): Re-raise task exceptions.
                Defaults to the :setting:`task_eager_propagates` setting.
    
        Returns:
            celery.result.EagerResult: pre-evaluated result.
        """
        # trace imports Task, so need to import inline.
        from celery.app.trace import build_tracer
    
        app = self._get_app()
        args = args or ()
        kwargs = kwargs or {}
        task_id = task_id or uuid()
        retries = retries or 0
        if throw is None:
            throw = app.conf.task_eager_propagates
    
        parent_task = _task_stack.top
        if parent_task and parent_task.request:
            parent_id = parent_task.request.id
            root_id = parent_task.request.root_id or task_id
        else:
            parent_id = None
            root_id = task_id
    
        # Make sure we get the task instance, not class.
        task = app._tasks[self.name]
    
        request = {
            'id': task_id,
            'task': self.name,
            'parent_id': parent_id,
            'root_id': root_id,
            'retries': retries,
            'is_eager': True,
            'logfile': logfile,
            'loglevel': loglevel or 0,
            'hostname': gethostname(),
            'callbacks': maybe_list(link),
            'errbacks': maybe_list(link_error),
            'headers': headers,
            'ignore_result': options.get('ignore_result', False),
            'delivery_info': {
                'is_eager': True,
                'exchange': options.get('exchange'),
                'routing_key': options.get('routing_key'),
                'priority': options.get('priority'),
            }
        }
        if 'stamped_headers' in options:
            request['stamped_headers'] = maybe_list(options['stamped_headers'])
            request['stamps'] = {
                header: maybe_list(options.get(header, [])) for header in request['stamped_headers']
            }
    
        tb = None
        tracer = build_tracer(
            task.name, task, eager=True,
            propagate=throw, app=self._get_app(),
        )
>       ret = tracer(task_id, args, kwargs, request)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/celery/app/task.py:843: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

uuid = '8c2b225b-9e41-4807-beb2-9b75309904d8'
args = ['doc_313242de275f5512', 'integration_test_tenant'], kwargs = {}
request = {'callbacks': None, 'delivery_info': {'exchange': None, 'is_eager': True, 'priority': None, 'routing_key': None}, 'errbacks': None, 'headers': None, ...}

    def trace_task(uuid, args, kwargs, request=None):
        # R      - is the possibly prepared return value.
        # I      - is the Info object.
        # T      - runtime
        # Rstr   - textual representation of return value
        # retval - is the always unmodified return value.
        # state  - is the resulting task state.
    
        # This function is very long because we've unrolled all the calls
        # for performance reasons, and because the function is so long
        # we want the main variables (I, and R) to stand out visually from the
        # the rest of the variables, so breaking PEP8 is worth it ;)
        R = I = T = Rstr = retval = state = None
        task_request = None
        time_start = monotonic()
        try:
            try:
                kwargs.items
            except AttributeError:
                raise InvalidTaskError(
                    'Task keyword arguments is not a mapping')
    
            task_request = Context(request or {}, args=args,
                                   called_directly=False, kwargs=kwargs)
    
            redelivered = (task_request.delivery_info
                           and task_request.delivery_info.get('redelivered', False))
            if deduplicate_successful_tasks and redelivered:
                if task_request.id in successful_requests:
                    return trace_ok_t(R, I, T, Rstr)
                r = AsyncResult(task_request.id, app=app)
    
                try:
                    state = r.state
                except BackendGetMetaError:
                    pass
                else:
                    if state == SUCCESS:
                        info(LOG_IGNORED, {
                            'id': task_request.id,
                            'name': get_task_name(task_request, name),
                            'description': 'Task already completed successfully.'
                        })
                        return trace_ok_t(R, I, T, Rstr)
    
            push_task(task)
            root_id = task_request.root_id or uuid
            task_priority = task_request.delivery_info.get('priority') if \
                inherit_parent_priority else None
            push_request(task_request)
            try:
                # -*- PRE -*-
                if prerun_receivers:
                    send_prerun(sender=task, task_id=uuid, task=task,
                                args=args, kwargs=kwargs)
                loader_task_init(uuid, task)
                if track_started:
                    task.backend.store_result(
                        uuid, {'pid': pid, 'hostname': hostname}, STARTED,
                        request=task_request,
                    )
    
                # -*- TRACE -*-
                try:
                    if task_before_start:
                        task_before_start(uuid, args, kwargs)
    
                    R = retval = fun(*args, **kwargs)
                    state = SUCCESS
                except Reject as exc:
                    I, R = Info(REJECTED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_reject(task, task_request)
                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)
                    traceback_clear(exc)
                except Ignore as exc:
                    I, R = Info(IGNORED, exc), ExceptionInfo(internal=True)
                    state, retval = I.state, I.retval
                    I.handle_ignore(task, task_request)
                    # MEMORY LEAK FIX: Clear traceback frames to prevent memory retention (Issue #8882)
                    traceback_clear(exc)
                except Retry as exc:
>                   I, R, state, retval = on_error(
                        task_request, exc, RETRY, call_errbacks=False)

.venv/lib/python3.12/site-packages/celery/app/trace.py:494: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

uuid = '8c2b225b-9e41-4807-beb2-9b75309904d8'
args = ['doc_313242de275f5512', 'integration_test_tenant'], kwargs = {}
request = {'callbacks': None, 'delivery_info': {'exchange': None, 'is_eager': True, 'priority': None, 'routing_key': None}, 'errbacks': None, 'headers': None, ...}

    def trace_task(uuid, args, kwargs, request=None):
        # R      - is the possibly prepared return value.
        # I      - is the Info object.
        # T      - runtime
        # Rstr   - textual representation of return value
        # retval - is the always unmodified return value.
        # state  - is the resulting task state.
    
        # This function is very long because we've unrolled all the calls
        # for performance reasons, and because the function is so long
        # we want the main variables (I, and R) to stand out visually from the
        # the rest of the variables, so breaking PEP8 is worth it ;)
        R = I = T = Rstr = retval = state = None
        task_request = None
        time_start = monotonic()
        try:
            try:
                kwargs.items
            except AttributeError:
                raise InvalidTaskError(
                    'Task keyword arguments is not a mapping')
    
            task_request = Context(request or {}, args=args,
                                   called_directly=False, kwargs=kwargs)
    
            redelivered = (task_request.delivery_info
                           and task_request.delivery_info.get('redelivered', False))
            if deduplicate_successful_tasks and redelivered:
                if task_request.id in successful_requests:
                    return trace_ok_t(R, I, T, Rstr)
                r = AsyncResult(task_request.id, app=app)
    
                try:
                    state = r.state
                except BackendGetMetaError:
                    pass
                else:
                    if state == SUCCESS:
                        info(LOG_IGNORED, {
                            'id': task_request.id,
                            'name': get_task_name(task_request, name),
                            'description': 'Task already completed successfully.'
                        })
                        return trace_ok_t(R, I, T, Rstr)
    
            push_task(task)
            root_id = task_request.root_id or uuid
            task_priority = task_request.delivery_info.get('priority') if \
                inherit_parent_priority else None
            push_request(task_request)
            try:
                # -*- PRE -*-
                if prerun_receivers:
                    send_prerun(sender=task, task_id=uuid, task=task,
                                args=args, kwargs=kwargs)
                loader_task_init(uuid, task)
                if track_started:
                    task.backend.store_result(
                        uuid, {'pid': pid, 'hostname': hostname}, STARTED,
                        request=task_request,
                    )
    
                # -*- TRACE -*-
                try:
                    if task_before_start:
                        task_before_start(uuid, args, kwargs)
    
>                   R = retval = fun(*args, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/celery/app/trace.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('doc_313242de275f5512', 'integration_test_tenant'), kwargs = {}

    @wraps(task.run)
    def run(*args, **kwargs):
        try:
>           return task._orig_run(*args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/celery/app/autoretry.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <@task: src.workers.tasks.process_document of graphrag at 0x7eb66151a960>
document_id = 'doc_313242de275f5512', tenant_id = 'integration_test_tenant'

    @celery_app.task(
        bind=True,
        name="src.workers.tasks.process_document",
        base=BaseTask,
        max_retries=3,
        default_retry_delay=60,
        queue="high_priority",
    )
    def process_document(self, document_id: str, tenant_id: str) -> dict:
        """
        Process a document through the full ingestion pipeline.
    
        Steps:
        1. Fetch document from DB
        2. Update status to EXTRACTING
        3. Run extraction (FallbackManager)
        4. Update status to CLASSIFYING
        5. Run domain classification
        6. Update status to CHUNKING
        7. Run semantic chunking
        8. Update status to READY
    
        Args:
            document_id: ID of the document to process.
            tenant_id: Tenant for context.
    
        Returns:
            dict: Processing result summary.
        """
        logger.info(f"[Task {self.request.id}] Starting processing for document {document_id}")
    
        try:
            result = run_async(_process_document_async(document_id, tenant_id, self.request.id))
            logger.info(f"[Task {self.request.id}] Completed processing for document {document_id}")
    
            # Trigger community detection asynchronously
            try:
                logger.info(
                    f"[Task {self.request.id}] Triggering community detection for tenant {tenant_id}"
                )
                process_communities.delay(tenant_id)
            except Exception as e:
                logger.warning(f"Failed to trigger community detection: {e}")
    
            return result
    
        except Exception as e:
            import traceback
    
            logger.error(
                f"[Task {self.request.id}] Failed processing document {document_id}: {e}\n{traceback.format_exc()}"
            )
    
            # Update document status to FAILED
            try:
                run_async(_mark_document_failed(document_id, str(e)))
            except Exception as fail_err:
                logger.error(f"Failed to mark document as failed: {fail_err}")
    
            # Retry if not exceeded
            try:
>               raise self.retry(exc=e)
                      ^^^^^^^^^^^^^^^^^

src/workers/tasks.py:173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <@task: src.workers.tasks.process_document of graphrag at 0x7eb66151a960>
args = None, kwargs = None
exc = RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)
throw = True, eta = None, countdown = 60, max_retries = 3, options = {}
request = <Context: {'id': '8c2b225b-9e41-4807-beb2-9b75309904d8', 'task': 'src.workers.tasks.process_document', 'parent_id': No...priority': None}, 'args': ['doc_313242de275f5512', 'integration_test_tenant'], 'called_directly': False, 'kwargs': {}}>
retries = 1, is_eager = True
S = src.workers.tasks.process_document('doc_313242de275f5512', 'integration_test_tenant')
ret = Retry(Retry(...), RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>), 60)

    def retry(self, args=None, kwargs=None, exc=None, throw=True,
              eta=None, countdown=None, max_retries=None, **options):
        """Retry the task, adding it to the back of the queue.
    
        Example:
            >>> from imaginary_twitter_lib import Twitter
            >>> from proj.celery import app
    
            >>> @app.task(bind=True)
            ... def tweet(self, auth, message):
            ...     twitter = Twitter(oauth=auth)
            ...     try:
            ...         twitter.post_status_update(message)
            ...     except twitter.FailWhale as exc:
            ...         # Retry in 5 minutes.
            ...         raise self.retry(countdown=60 * 5, exc=exc)
    
        Note:
            Although the task will never return above as `retry` raises an
            exception to notify the worker, we use `raise` in front of the
            retry to convey that the rest of the block won't be executed.
    
        Arguments:
            args (Tuple): Positional arguments to retry with.
            kwargs (Dict): Keyword arguments to retry with.
            exc (Exception): Custom exception to report when the max retry
                limit has been exceeded (default:
                :exc:`~@MaxRetriesExceededError`).
    
                If this argument is set and retry is called while
                an exception was raised (``sys.exc_info()`` is set)
                it will attempt to re-raise the current exception.
    
                If no exception was raised it will raise the ``exc``
                argument provided.
            countdown (float): Time in seconds to delay the retry for.
            eta (~datetime.datetime): Explicit time and date to run the
                retry at.
            max_retries (int): If set, overrides the default retry limit for
                this execution.  Changes to this parameter don't propagate to
                subsequent task retry attempts.  A value of :const:`None`,
                means "use the default", so if you want infinite retries you'd
                have to set the :attr:`max_retries` attribute of the task to
                :const:`None` first.
            time_limit (int): If set, overrides the default time limit.
            soft_time_limit (int): If set, overrides the default soft
                time limit.
            throw (bool): If this is :const:`False`, don't raise the
                :exc:`~@Retry` exception, that tells the worker to mark
                the task as being retried.  Note that this means the task
                will be marked as failed if the task raises an exception,
                or successful if it returns after the retry call.
            **options (Any): Extra options to pass on to :meth:`apply_async`.
    
        Raises:
    
            celery.exceptions.Retry:
                To tell the worker that the task has been re-sent for retry.
                This always happens, unless the `throw` keyword argument
                has been explicitly set to :const:`False`, and is considered
                normal operation.
        """
        request = self.request
        retries = request.retries + 1
        if max_retries is not None:
            self.override_max_retries = max_retries
        max_retries = self.max_retries if max_retries is None else max_retries
    
        # Not in worker or emulated by (apply/always_eager),
        # so just raise the original exception.
        if request.called_directly:
            # raises orig stack if PyErr_Occurred,
            # and augments with exc' if that argument is defined.
            raise_with_context(exc or Retry('Task can be retried', None))
    
        if not eta and countdown is None:
            countdown = self.default_retry_delay
    
        is_eager = request.is_eager
        S = self.signature_from_request(
            request, args, kwargs,
            countdown=countdown, eta=eta, retries=retries,
            **options
        )
    
        if max_retries is not None and retries > max_retries:
            if exc:
                # On Py3: will augment any current exception with
                # the exc' argument provided (raise exc from orig)
                raise_with_context(exc)
            raise self.MaxRetriesExceededError(
                "Can't retry {}[{}] args:{} kwargs:{}".format(
                    self.name, request.id, S.args, S.kwargs
                ), task_args=S.args, task_kwargs=S.kwargs
            )
    
        ret = Retry(exc=exc, when=eta or countdown, is_eager=is_eager, sig=S)
    
        if is_eager:
            # if task was executed eagerly using apply(),
            # then the retry must also be executed eagerly in apply method
            if throw:
>               raise ret
E               celery.exceptions.Retry: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)

.venv/lib/python3.12/site-packages/celery/app/task.py:763: Retry

The above exception was the direct cause of the following exception:

self = <tests.integration.test_ingestion_pipeline.TestIngestionPipeline object at 0x7eb660958290>
client = <httpx.AsyncClient object at 0x7eb629433f50>
api_key = 'amber-dev-key-2024'
test_pdf_file = ('test_integration_5fd01fbb.pdf', b'%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n2 0 obj\n<<\n/Typ...iler\n<<\n/Size 6\n/Root 1 0 R\n>>\nstartxref\n882\n%%EOF\n\n% Random: 5fd01fbb 1771003242.5259762', 'application/pdf')

    @pytest.mark.asyncio
    async def test_complete_pipeline(
        self, client: AsyncClient, api_key: str, test_pdf_file: tuple[str, bytes, str]
    ):
        """
        Test the complete ingestion pipeline from upload to graph sync.
    
        This test verifies:
        1. Document upload succeeds
        2. Document processes through all stages
        3. Chunks are created and embedded
        4. Entities are extracted
        5. Relationships are created
        6. Data exists in Neo4j
        7. Embeddings exist in Milvus
        """
        filename, content, content_type = test_pdf_file
    
        # Force eager execution to run tasks synchronously in the test process
        from src.workers.celery_app import celery_app
    
        celery_app.conf.task_always_eager = True
        celery_app.conf.task_eager_propagates = True
    
        # Step 0: Verify Tenant Isolation
        # We rely on the client injection of X-Tenant-ID
        # The API key service mock/fixture should have set this up.
    
        # Verify that we are NOT touching the default collection manually
        print("\n0. Pipeline running under tenant isolation.")
    
        # REMOVED: Manual drop_collection on custom names.
        # We rely on the 'cleanup_test_tenant' fixture in conftest.py matches this.
        # The API will use "amber_{tenant_id}" automatically.
    
        # Step 1: Upload document
        print("\n1. Uploading document...")
        files = {"file": (filename, content, content_type)}
    
        # Mock process_communities to avoid blocking on it during eager execution
        # We will trigger it manually later if needed
        from unittest.mock import patch
    
        with patch("src.workers.tasks.process_communities"):
>           response = await client.post(
                "/v1/documents", headers={"X-API-Key": api_key}, files=files
            )

tests/integration/test_ingestion_pipeline.py:207: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>, url = '/v1/documents'

    async def post(
        self,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Send a `POST` request.
    
        **Parameters**: See `httpx.request`.
        """
>       return await self.request(
            "POST",
            url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            auth=auth,
            follow_redirects=follow_redirects,
            timeout=timeout,
            extensions=extensions,
        )

.venv/lib/python3.12/site-packages/httpx/_client.py:1859: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>, method = 'POST'
url = '/v1/documents'

    async def request(
        self,
        method: str,
        url: URL | str,
        *,
        content: RequestContent | None = None,
        data: RequestData | None = None,
        files: RequestFiles | None = None,
        json: typing.Any | None = None,
        params: QueryParamTypes | None = None,
        headers: HeaderTypes | None = None,
        cookies: CookieTypes | None = None,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
        timeout: TimeoutTypes | UseClientDefault = USE_CLIENT_DEFAULT,
        extensions: RequestExtensions | None = None,
    ) -> Response:
        """
        Build and send a request.
    
        Equivalent to:
    
        ```python
        request = client.build_request(...)
        response = await client.send(request, ...)
        ```
    
        See `AsyncClient.build_request()`, `AsyncClient.send()`
        and [Merging of configuration][0] for how the various parameters
        are merged with client-level configuration.
    
        [0]: /advanced/clients/#merging-of-configuration
        """
    
        if cookies is not None:  # pragma: no cover
            message = (
                "Setting per-request cookies=<...> is being deprecated, because "
                "the expected behaviour on cookie persistence is ambiguous. Set "
                "cookies directly on the client instance instead."
            )
            warnings.warn(message, DeprecationWarning, stacklevel=2)
    
        request = self.build_request(
            method=method,
            url=url,
            content=content,
            data=data,
            files=files,
            json=json,
            params=params,
            headers=headers,
            cookies=cookies,
            timeout=timeout,
            extensions=extensions,
        )
>       return await self.send(request, auth=auth, follow_redirects=follow_redirects)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_client.py:1540: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>
request = <Request('POST', 'http://test/v1/documents')>

    async def send(
        self,
        request: Request,
        *,
        stream: bool = False,
        auth: AuthTypes | UseClientDefault | None = USE_CLIENT_DEFAULT,
        follow_redirects: bool | UseClientDefault = USE_CLIENT_DEFAULT,
    ) -> Response:
        """
        Send a request.
    
        The request is sent as-is, unmodified.
    
        Typically you'll want to build one with `AsyncClient.build_request()`
        so that any client-level configuration is merged into the request,
        but passing an explicit `httpx.Request()` is supported as well.
    
        See also: [Request instances][0]
    
        [0]: /advanced/clients/#request-instances
        """
        if self._state == ClientState.CLOSED:
            raise RuntimeError("Cannot send a request, as the client has been closed.")
    
        self._state = ClientState.OPENED
        follow_redirects = (
            self.follow_redirects
            if isinstance(follow_redirects, UseClientDefault)
            else follow_redirects
        )
    
        self._set_timeout(request)
    
        auth = self._build_request_auth(request, auth)
    
>       response = await self._send_handling_auth(
            request,
            auth=auth,
            follow_redirects=follow_redirects,
            history=[],
        )

.venv/lib/python3.12/site-packages/httpx/_client.py:1629: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>
request = <Request('POST', 'http://test/v1/documents')>
auth = <httpx.Auth object at 0x7eb629432ba0>, follow_redirects = False
history = []

    async def _send_handling_auth(
        self,
        request: Request,
        auth: Auth,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        auth_flow = auth.async_auth_flow(request)
        try:
            request = await auth_flow.__anext__()
    
            while True:
>               response = await self._send_handling_redirects(
                    request,
                    follow_redirects=follow_redirects,
                    history=history,
                )

.venv/lib/python3.12/site-packages/httpx/_client.py:1657: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>
request = <Request('POST', 'http://test/v1/documents')>
follow_redirects = False, history = []

    async def _send_handling_redirects(
        self,
        request: Request,
        follow_redirects: bool,
        history: list[Response],
    ) -> Response:
        while True:
            if len(history) > self.max_redirects:
                raise TooManyRedirects(
                    "Exceeded maximum allowed redirects.", request=request
                )
    
            for hook in self._event_hooks["request"]:
                await hook(request)
    
>           response = await self._send_single_request(request)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_client.py:1694: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.AsyncClient object at 0x7eb629433f50>
request = <Request('POST', 'http://test/v1/documents')>

    async def _send_single_request(self, request: Request) -> Response:
        """
        Sends a single request, without handling any redirections.
        """
        transport = self._transport_for_url(request.url)
        start = time.perf_counter()
    
        if not isinstance(request.stream, AsyncByteStream):
            raise RuntimeError(
                "Attempted to send an sync request with an AsyncClient instance."
            )
    
        with request_context(request=request):
>           response = await transport.handle_async_request(request)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/httpx/_client.py:1730: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <httpx.ASGITransport object at 0x7eb629433fb0>
request = <Request('POST', 'http://test/v1/documents')>

    async def handle_async_request(
        self,
        request: Request,
    ) -> Response:
        assert isinstance(request.stream, AsyncByteStream)
    
        # ASGI scope.
        scope = {
            "type": "http",
            "asgi": {"version": "3.0"},
            "http_version": "1.1",
            "method": request.method,
            "headers": [(k.lower(), v) for (k, v) in request.headers.raw],
            "scheme": request.url.scheme,
            "path": request.url.path,
            "raw_path": request.url.raw_path.split(b"?")[0],
            "query_string": request.url.query,
            "server": (request.url.host, request.url.port),
            "client": self.client,
            "root_path": self.root_path,
        }
    
        # Request.
        request_body_chunks = request.stream.__aiter__()
        request_complete = False
    
        # Response.
        status_code = None
        response_headers = None
        body_parts = []
        response_started = False
        response_complete = create_event()
    
        # ASGI callables.
    
        async def receive() -> dict[str, typing.Any]:
            nonlocal request_complete
    
            if request_complete:
                await response_complete.wait()
                return {"type": "http.disconnect"}
    
            try:
                body = await request_body_chunks.__anext__()
            except StopAsyncIteration:
                request_complete = True
                return {"type": "http.request", "body": b"", "more_body": False}
            return {"type": "http.request", "body": body, "more_body": True}
    
        async def send(message: typing.MutableMapping[str, typing.Any]) -> None:
            nonlocal status_code, response_headers, response_started
    
            if message["type"] == "http.response.start":
                assert not response_started
    
                status_code = message["status"]
                response_headers = message.get("headers", [])
                response_started = True
    
            elif message["type"] == "http.response.body":
                assert not response_complete.is_set()
                body = message.get("body", b"")
                more_body = message.get("more_body", False)
    
                if body and request.method != "HEAD":
                    body_parts.append(body)
    
                if not more_body:
                    response_complete.set()
    
        try:
>           await self.app(scope, receive, send)

.venv/lib/python3.12/site-packages/httpx/_transports/asgi.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.applications.FastAPI object at 0x7eb660e502f0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ASGITransport.handle_async_request.<locals>.send at 0x7eb62939b060>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.root_path:
            scope["root_path"] = self.root_path
>       await super().__call__(scope, receive, send)

.venv/lib/python3.12/site-packages/fastapi/applications.py:1135: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.applications.FastAPI object at 0x7eb660e502f0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ASGITransport.handle_async_request.<locals>.send at 0x7eb62939b060>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        scope["app"] = self
        if self.middleware_stack is None:
            self.middleware_stack = self.build_middleware_stack()
>       await self.middleware_stack(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/applications.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x7eb6293746b0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ASGITransport.handle_async_request.<locals>.send at 0x7eb62939b060>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await self.app(scope, receive, _send)
        except Exception as exc:
            request = Request(scope)
            if self.debug:
                # In debug mode, return traceback responses.
                response = self.debug_response(request, exc)
            elif self.handler is None:
                # Use our default 500 error handler.
                response = self.error_response(request, exc)
            else:
                # Use an installed 500 error handler.
                if is_async_callable(self.handler):
                    response = await self.handler(request, exc)
                else:
                    response = await run_in_threadpool(self.handler, request, exc)
    
            if not response_started:
                await response(scope, receive, send)
    
            # We always continue to raise the exception.
            # This allows servers to log the error, or allows test clients
            # to optionally raise the error within the test case.
>           raise exc

.venv/lib/python3.12/site-packages/starlette/middleware/errors.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <starlette.middleware.errors.ServerErrorMiddleware object at 0x7eb6293746b0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ASGITransport.handle_async_request.<locals>.send at 0x7eb62939b060>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        response_started = False
    
        async def _send(message: Message) -> None:
            nonlocal response_started, send
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await self.app(scope, receive, _send)

.venv/lib/python3.12/site-packages/starlette/middleware/errors.py:164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.request_id.RequestIdMiddleware object at 0x7eb629374500>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x7eb62939b100>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb6293761b0>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb6293f1ac0>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.request_id.RequestIdMiddleware object at 0x7eb629374500>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function ASGITransport.handle_async_request.<locals>.receive at 0x7eb62939ac00>
send = <function ServerErrorMiddleware.__call__.<locals>._send at 0x7eb62939b100>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.request_id.RequestIdMiddleware object at 0x7eb629374500>
request = <starlette.middleware.base._CachedRequest object at 0x7eb629375370>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939b1a0>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process the request through request ID handling."""
        # Check for client-provided request ID
        client_id = request.headers.get(REQUEST_ID_HEADER)
        request_id: str
    
        if client_id:
            # Validate client ID (must be valid UUID)
            try:
                uuid.UUID(client_id)
                request_id = client_id
            except ValueError:
                # Invalid UUID, generate a new one
                request_id = f"req_{uuid.uuid4().hex}"
        else:
            # Generate new request ID
            request_id = f"req_{uuid.uuid4().hex}"
    
        # Set in context
        set_request_id(RequestId(request_id))
    
        # Store in request state
        request.state.request_id = request_id
    
        # Process request
>       response = await call_next(request)
                   ^^^^^^^^^^^^^^^^^^^^^^^^

src/api/middleware/request_id.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb629375370>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.timing.TimingMiddleware object at 0x7eb629374350>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b2e0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939b380>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb629378b90>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb6293f1700>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.timing.TimingMiddleware object at 0x7eb629374350>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b2e0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939b380>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.timing.TimingMiddleware object at 0x7eb629374350>
request = <starlette.middleware.base._CachedRequest object at 0x7eb629378170>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939b560>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process the request through timing."""
        start_time = time.perf_counter()
    
>       response = await call_next(request)
                   ^^^^^^^^^^^^^^^^^^^^^^^^

src/api/middleware/timing.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb629378170>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.admin_ops.infrastructure.observability.middleware.StructuredLoggingMiddleware object at 0x7eb6293741d0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b600>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939b6a0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb62937bda0>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb5fa86af80>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.admin_ops.infrastructure.observability.middleware.StructuredLoggingMiddleware object at 0x7eb6293741d0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b600>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939b6a0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.admin_ops.infrastructure.observability.middleware.StructuredLoggingMiddleware object at 0x7eb6293741d0>
request = <starlette.middleware.base._CachedRequest object at 0x7eb62937b3e0>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939b920>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        start_time = time.perf_counter()
    
        path = request.url.path
        method = request.method
    
        # Skip health checks to avoid log noise
        if path.endswith("/health") or path.endswith("/ready"):
            return await call_next(request)
    
        try:
            response = await call_next(request)
    
            latency = (time.perf_counter() - start_time) * 1000
    
            log_data = {
                "method": method,
                "path": path,
                "status_code": response.status_code,
                "latency_ms": round(latency, 2),
                "ip": request.client.host if request.client else None,
            }
    
            # Log level depends on status code
            if response.status_code >= 500:
                logger.error("Request failed", extra={"props": log_data})
            elif response.status_code >= 400:
                logger.warning("Request bad input", extra={"props": log_data})
            else:
                logger.info("Request processed", extra={"props": log_data})
    
            return response
    
        except Exception as e:
            latency = (time.perf_counter() - start_time) * 1000
            logger.error(
                f"Request exception: {str(e)}",
                extra={
                    "props": {
                        "method": method,
                        "path": path,
                        "status_code": 500,
                        "latency_ms": round(latency, 2),
                    }
                },
            )
>           raise e

src/core/admin_ops/infrastructure/observability/middleware.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.admin_ops.infrastructure.observability.middleware.StructuredLoggingMiddleware object at 0x7eb6293741d0>
request = <starlette.middleware.base._CachedRequest object at 0x7eb62937b3e0>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939b920>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        start_time = time.perf_counter()
    
        path = request.url.path
        method = request.method
    
        # Skip health checks to avoid log noise
        if path.endswith("/health") or path.endswith("/ready"):
            return await call_next(request)
    
        try:
>           response = await call_next(request)
                       ^^^^^^^^^^^^^^^^^^^^^^^^

src/core/admin_ops/infrastructure/observability/middleware.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb62937b3e0>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.RateLimitMiddleware object at 0x7eb629367fe0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b9c0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939ba60>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb629382db0>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb62895ca00>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.RateLimitMiddleware object at 0x7eb629367fe0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb62939b9c0>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939ba60>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.RateLimitMiddleware object at 0x7eb629367fe0>
request = <starlette.middleware.base._CachedRequest object at 0x7eb629382390>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939bd80>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process the request through rate limiting."""
        path = request.url.path
        method = request.method
        origin = request.headers.get("Origin", "*")
    
        # Determine rate limit category
        category = _get_category(path, method)
        if category is None:
            return await call_next(request)
    
        # Get tenant ID (set by auth middleware)
        tenant_id = get_current_tenant()
        if tenant_id is None:
            # If no tenant (before auth), use IP address as identifier
            tenant_id = request.client.host or "anonymous"
    
        # Check rate limit
        try:
            result = await _get_rate_limiter().check(str(tenant_id), category)
        except Exception as e:
            # Fail open if rate limiter fails (e.g. Redis down)
            logger.warning(f"Rate limiter failed (fail open): {e}")
            return await call_next(request)
    
        if not result.allowed:
            logger.warning(
                f"Rate limit exceeded: tenant={tenant_id}, category={category.value}, "
                f"path={method} {path}"
            )
            response = JSONResponse(
                status_code=429,
                content={
                    "error": {
                        "code": "RATE_LIMIT_EXCEEDED",
                        "message": f"Too many requests. Please retry after {result.retry_after} seconds.",
                        "retry_after": result.retry_after,
                    }
                },
                headers={
                    "Retry-After": str(result.retry_after),
                    "X-RateLimit-Limit": str(result.limit),
                    "X-RateLimit-Remaining": "0",
                    "X-RateLimit-Reset": str(result.reset_at),
                },
            )
            return _add_cors_headers(response, origin)
    
        # Proceed with request
>       response = await call_next(request)
                   ^^^^^^^^^^^^^^^^^^^^^^^^

src/api/middleware/rate_limit.py:135: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb629382390>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.auth.AuthenticationMiddleware object at 0x7eb629367b00>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb6608e3c40>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939bf60>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb629322bd0>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb628975500>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.auth.AuthenticationMiddleware object at 0x7eb629367b00>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb6608e3c40>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb62939bf60>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.auth.AuthenticationMiddleware object at 0x7eb629367b00>
request = <starlette.middleware.base._CachedRequest object at 0x7eb62939ec90>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb628944180>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process the request through authentication."""
        path = request.url.path
        origin = request.headers.get("Origin", "*")
    
        # Allow CORS preflight requests through without auth
        if request.method == "OPTIONS":
            return await call_next(request)
    
        # Skip auth for public paths
        if _is_public_path(path):
            return await call_next(request)
    
        api_key = None
    
        # 1. Try Ticket Auth (Secure SSE)
        ticket = request.query_params.get("ticket")
        is_sse_path = any(p in path for p in ["/stream", "/events"])
    
        if ticket and is_sse_path:
            from src.core.auth.application.ticket_service import TicketService
    
            ticket_service = TicketService()
            try:
                # Redeem ticket (consume it)
                api_key = await ticket_service.redeem_ticket(ticket)
                if not api_key:
                    logger.warning(f"Invalid or expired ticket used for {path}")
                    return _cors_error_response(
                        401, "UNAUTHORIZED", "Invalid or expired ticket.", origin
                    )
            except Exception as e:
                logger.error(f"Ticket redemption error: {e}")
                return _cors_error_response(500, "INTERNAL_ERROR", "Auth error", origin)
            finally:
                await ticket_service.close()
    
        # 2. Try Standard Header Auth
        if not api_key:
            api_key = request.headers.get("X-API-Key")
    
        # 3. Fallback: Legacy Query Param for SSE (Deprecated but kept for compat if needed,
        #    but we prefer Ticket now. We can log a warning if used.)
        if not api_key and is_sse_path:
            api_key = request.query_params.get("api_key")
            if api_key:
                logger.warning(f"Legacy query param auth used for {path}. migrate to ticket auth.")
    
        if not api_key:
            logger.warning(f"Missing API key for {request.method} {path}")
            return _cors_error_response(
                401,
                "UNAUTHORIZED",
                "Missing API key. Provide X-API-Key header or valid ticket.",
                origin,
            )
    
        # Validate API key via Service
        from src.api.deps import _get_async_session_maker
        from src.core.admin_ops.application.api_key_service import ApiKeyService
    
        valid_key = None
        try:
            async with _get_async_session_maker()() as session:
                service = ApiKeyService(session)
                valid_key = await service.validate_key(api_key)
        except Exception as e:
            logger.error(f"Auth DB Error: {e}")
            return _cors_error_response(500, "INTERNAL_ERROR", "Authentication failed", origin)
    
        if not valid_key:
            logger.warning(f"Invalid API key {mask_api_key(api_key)} for {request.method} {path}")
            return _cors_error_response(401, "UNAUTHORIZED", "Invalid API key.", origin)
    
        # Resolve Tenant Context
        header_tenant_id = request.headers.get("X-Tenant-ID")
        allowed_tenants = {t.id for t in valid_key.tenants}
        tenant_id = None
    
        if header_tenant_id:
            # Client requested specific tenant
            if header_tenant_id in allowed_tenants:
                tenant_id = TenantId(header_tenant_id)
            elif "super_admin" in (valid_key.scopes or []) or "root" in (
                valid_key.scopes or []
            ):  # Allow Super Admin to impersonate any tenant
                tenant_id = TenantId(header_tenant_id)
            elif not allowed_tenants:
                # Legacy/Bootstrap: If key has no specific links, allow 'default' if requested
                # This ensures unmigrated keys still work for default tenant
                if header_tenant_id == "default":
                    tenant_id = TenantId("default")
                else:
                    logger.warning(
                        f"Access denied for key {valid_key.name} to tenant {header_tenant_id} (No links)"
                    )
                    return _cors_error_response(403, "FORBIDDEN", "Access to tenant denied", origin)
            else:
                logger.warning(
                    f"Access denied for key {valid_key.name} to tenant {header_tenant_id}"
                )
                return _cors_error_response(403, "FORBIDDEN", "Access to tenant denied", origin)
        else:
            # No tenant specified
            if len(allowed_tenants) == 1:
                # Ambiguity resolved: exact one match
                tenant_id = TenantId(list(allowed_tenants)[0])
            elif not allowed_tenants:
                # Fallback to default
                tenant_id = TenantId("default")
            else:
                # Ambiguous
                return _cors_error_response(
                    400,
                    "BAD_REQUEST",
                    "Multiple tenants available. Specify X-Tenant-ID header.",
                    origin,
                )
    
        permissions = valid_key.scopes or []
    
        set_current_tenant(tenant_id)
        set_permissions(permissions)
    
        # Resolve Tenant Role from the ApiKeyTenant association
        tenant_role = "user"  # Default role
        if str(tenant_id) in allowed_tenants:
            # Find the specific association to get the role
            from src.core.admin_ops.domain.api_key import ApiKeyTenant
    
            async with _get_async_session_maker()() as session:
                from sqlalchemy import select
    
                result = await session.execute(
                    select(ApiKeyTenant.role).where(
                        ApiKeyTenant.api_key_id == valid_key.id,
                        ApiKeyTenant.tenant_id == str(tenant_id),
                    )
                )
                role_row = result.scalar_one_or_none()
                if role_row:
                    tenant_role = role_row
    
        # Store in request state for easy access
        request.state.tenant_id = tenant_id
        request.state.permissions = permissions
        request.state.api_key_name = valid_key.name
        request.state.tenant_role = tenant_role
        request.state.is_super_admin = "super_admin" in permissions
    
        logger.debug(
            f"Authenticated: tenant={tenant_id}, key={valid_key.name}, "
            f"role={tenant_role}, super_admin={request.state.is_super_admin}, "
            f"path={request.method} {path}"
        )
    
>       return await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^

src/api/middleware/auth.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb62939ec90>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.UploadSizeLimitMiddleware object at 0x7eb629367980>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944d60>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628945440>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
>       with recv_stream, send_stream, collapse_excgroups():

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:191: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7eb62894c320>
typ = <class 'ExceptionGroup'>
value = ExceptionGroup('unhandled errors in a TaskGroup', [RuntimeError('Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)')])
traceback = <traceback object at 0x7eb5fa9686c0>

    def __exit__(self, typ, value, traceback):
        if typ is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                try:
                    raise RuntimeError("generator didn't stop")
                finally:
                    self.gen.close()
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = typ()
            try:
>               self.gen.throw(value)

/usr/lib/python3.12/contextlib.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @contextmanager
    def collapse_excgroups() -> Generator[None, None, None]:
        try:
            yield
        except BaseException as exc:
            if has_exceptiongroups:  # pragma: no cover
                while isinstance(exc, BaseExceptionGroup) and len(exc.exceptions) == 1:
                    exc = exc.exceptions[0]
    
>           raise exc

.venv/lib/python3.12/site-packages/starlette/_utils.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.UploadSizeLimitMiddleware object at 0x7eb629367980>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944d60>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628945440>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
    
        request = _CachedRequest(scope, receive)
        wrapped_receive = request.wrapped_receive
        response_sent = anyio.Event()
        app_exc: Exception | None = None
        exception_already_raised = False
    
        async def call_next(request: Request) -> Response:
            async def receive_or_disconnect() -> Message:
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                async with anyio.create_task_group() as task_group:
    
                    async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                        result = await func()
                        task_group.cancel_scope.cancel()
                        return result
    
                    task_group.start_soon(wrap, response_sent.wait)
                    message = await wrap(wrapped_receive)
    
                if response_sent.is_set():
                    return {"type": "http.disconnect"}
    
                return message
    
            async def send_no_error(message: Message) -> None:
                try:
                    await send_stream.send(message)
                except anyio.BrokenResourceError:
                    # recv_stream has been closed, i.e. response_sent has been set.
                    return
    
            async def coro() -> None:
                nonlocal app_exc
    
                with send_stream:
                    try:
                        await self.app(scope, receive_or_disconnect, send_no_error)
                    except Exception as exc:
                        app_exc = exc
    
            task_group.start_soon(coro)
    
            try:
                message = await recv_stream.receive()
                info = message.get("info", None)
                if message["type"] == "http.response.debug" and info is not None:
                    message = await recv_stream.receive()
            except anyio.EndOfStream:
                if app_exc is not None:
                    nonlocal exception_already_raised
                    exception_already_raised = True
                    # Prevent `anyio.EndOfStream` from polluting app exception context.
                    # If both cause and context are None then the context is suppressed
                    # and `anyio.EndOfStream` is not present in the exception traceback.
                    # If exception cause is not None then it is propagated with
                    # reraising here.
                    # If exception has no cause but has context set then the context is
                    # propagated as a cause with the reraise. This is necessary in order
                    # to prevent `anyio.EndOfStream` from polluting the exception
                    # context.
                    raise app_exc from app_exc.__cause__ or app_exc.__context__
                raise RuntimeError("No response returned.")
    
            assert message["type"] == "http.response.start"
    
            async def body_stream() -> BodyStreamGenerator:
                async for message in recv_stream:
                    if message["type"] == "http.response.pathsend":
                        yield message
                        break
                    assert message["type"] == "http.response.body", f"Unexpected message: {message}"
                    body = message.get("body", b"")
                    if body:
                        yield body
                    if not message.get("more_body", False):
                        break
    
            response = _StreamingResponse(status_code=message["status"], content=body_stream(), info=info)
            response.raw_headers = message["headers"]
            return response
    
        streams: anyio.create_memory_object_stream[Message] = anyio.create_memory_object_stream()
        send_stream, recv_stream = streams
        with recv_stream, send_stream, collapse_excgroups():
            async with anyio.create_task_group() as task_group:
>               response = await self.dispatch_func(request, call_next)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.api.middleware.rate_limit.UploadSizeLimitMiddleware object at 0x7eb629367980>
request = <starlette.middleware.base._CachedRequest object at 0x7eb62891be30>
call_next = <function BaseHTTPMiddleware.__call__.<locals>.call_next at 0x7eb62939b240>

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process the request through size limiting."""
        origin = request.headers.get("Origin", "*")
    
        # Only check POST/PUT requests
        if request.method not in ("POST", "PUT", "PATCH"):
            return await call_next(request)
    
        content_length = request.headers.get("content-length")
        if content_length:
            try:
                size_bytes = int(content_length)
                max_bytes = settings.uploads.max_size_mb * 1024 * 1024
    
                if size_bytes > max_bytes:
                    logger.warning(
                        f"Upload too large: {size_bytes} bytes > {max_bytes} bytes "
                        f"(path={request.method} {request.url.path})"
                    )
                    response = JSONResponse(
                        status_code=413,
                        content={
                            "error": {
                                "code": "PAYLOAD_TOO_LARGE",
                                "message": f"Upload exceeds maximum size of {settings.uploads.max_size_mb}MB",
                                "max_size_mb": settings.uploads.max_size_mb,
                                "received_mb": round(size_bytes / (1024 * 1024), 2),
                            }
                        },
                    )
                    return _add_cors_headers(response, origin)
            except ValueError:
                pass  # Invalid content-length, let FastAPI handle it
    
>       return await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^

src/api/middleware/rate_limit.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.middleware.base._CachedRequest object at 0x7eb62891be30>

    async def call_next(request: Request) -> Response:
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: Callable[[], Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(wrapped_receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(coro)
    
        try:
            message = await recv_stream.receive()
            info = message.get("info", None)
            if message["type"] == "http.response.debug" and info is not None:
                message = await recv_stream.receive()
        except anyio.EndOfStream:
            if app_exc is not None:
                nonlocal exception_already_raised
                exception_already_raised = True
                # Prevent `anyio.EndOfStream` from polluting app exception context.
                # If both cause and context are None then the context is suppressed
                # and `anyio.EndOfStream` is not present in the exception traceback.
                # If exception cause is not None then it is propagated with
                # reraising here.
                # If exception has no cause but has context set then the context is
                # propagated as a cause with the reraise. This is necessary in order
                # to prevent `anyio.EndOfStream` from polluting the exception
                # context.
>               raise app_exc from app_exc.__cause__ or app_exc.__context__

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def coro() -> None:
        nonlocal app_exc
    
        with send_stream:
            try:
>               await self.app(scope, receive_or_disconnect, send_no_error)

.venv/lib/python3.12/site-packages/starlette/middleware/base.py:144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <starlette.middleware.cors.CORSMiddleware object at 0x7eb629367650>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628944220>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "http":  # pragma: no cover
            await self.app(scope, receive, send)
            return
    
        method = scope["method"]
        headers = Headers(scope=scope)
        origin = headers.get("origin")
    
        if origin is None:
>           await self.app(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/middleware/cors.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <starlette.middleware.exceptions.ExceptionMiddleware object at 0x7eb6293674a0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628944220>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] not in ("http", "websocket"):
            await self.app(scope, receive, send)
            return
    
        scope["starlette.exception_handlers"] = (
            self._exception_handlers,
            self._status_handlers,
        )
    
        conn: Request | WebSocket
        if scope["type"] == "http":
            conn = Request(scope, receive, send)
        else:
            conn = WebSocket(scope, receive, send)
    
>       await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628944220>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv/lib/python3.12/site-packages/starlette/_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.send_no_error at 0x7eb628944220>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv/lib/python3.12/site-packages/starlette/_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.middleware.asyncexitstack.AsyncExitStackMiddleware object at 0x7eb629367320>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        async with AsyncExitStack() as stack:
            scope[self.context_name] = stack
>           await self.app(scope, receive, send)

.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py:18: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.routing.APIRouter object at 0x7eb660e503e0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        """
        The main entry point to the Router class.
        """
>       await self.middleware_stack(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/routing.py:716: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.routing.APIRouter object at 0x7eb660e503e0>
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def app(self, scope: Scope, receive: Receive, send: Send) -> None:
        assert scope["type"] in ("http", "websocket", "lifespan")
    
        if "router" not in scope:
            scope["router"] = self
    
        if scope["type"] == "lifespan":
            await self.lifespan(scope, receive, send)
            return
    
        partial = None
    
        for route in self.routes:
            # Determine if any route matches the incoming scope,
            # and hand over to the matching route if found.
            match, child_scope = route.matches(scope)
            if match == Match.FULL:
                scope.update(child_scope)
>               await route.handle(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/routing.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = APIRoute(path='/v1/documents', name='upload_document', methods=['POST'])
scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:
        if self.methods and scope["method"] not in self.methods:
            headers = {"Allow": ", ".join(self.methods)}
            if "app" in scope:
                raise HTTPException(status_code=405, headers=headers)
            else:
                response = PlainTextResponse("Method Not Allowed", status_code=405, headers=headers)
            await response(scope, receive, send)
        else:
>           await self.app(scope, receive, send)

.venv/lib/python3.12/site-packages/starlette/routing.py:290: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
        request = Request(scope, receive, send)
    
        async def app(scope: Scope, receive: Receive, send: Send) -> None:
            # Starts customization
            response_awaited = False
            async with AsyncExitStack() as request_stack:
                scope["fastapi_inner_astack"] = request_stack
                async with AsyncExitStack() as function_stack:
                    scope["fastapi_function_astack"] = function_stack
                    response = await f(request)
                await response(scope, receive, send)
                # Continues customization
                response_awaited = True
            if not response_awaited:
                raise FastAPIError(
                    "Response not awaited. There's a high chance that the "
                    "application code is raising an exception and a dependency with yield "
                    "has a block with a bare except, or a block with except Exception, "
                    "and is not raising the exception again. Read more about it in the "
                    "docs: https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#dependencies-with-yield-and-except"
                )
    
        # Same as in Starlette
>       await wrap_app_handling_exceptions(app, request)(scope, receive, send)

.venv/lib/python3.12/site-packages/fastapi/routing.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
            await app(scope, receive, sender)
        except Exception as exc:
            handler = None
    
            if isinstance(exc, HTTPException):
                handler = status_handlers.get(exc.status_code)
    
            if handler is None:
                handler = _lookup_exception_handler(exception_handlers, exc)
    
            if handler is None:
>               raise exc

.venv/lib/python3.12/site-packages/starlette/_exception_handler.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb6289454e0>

    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:
        response_started = False
    
        async def sender(message: Message) -> None:
            nonlocal response_started
    
            if message["type"] == "http.response.start":
                response_started = True
            await send(message)
    
        try:
>           await app(scope, receive, sender)

.venv/lib/python3.12/site-packages/starlette/_exception_handler.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

scope = {'app': <fastapi.applications.FastAPI object at 0x7eb660e502f0>, 'asgi': {'version': '3.0'}, 'client': ('127.0.0.1', 123), 'endpoint': <function upload_document at 0x7eb660ea0040>, ...}
receive = <function BaseHTTPMiddleware.__call__.<locals>.call_next.<locals>.receive_or_disconnect at 0x7eb628944360>
send = <function wrap_app_handling_exceptions.<locals>.wrapped_app.<locals>.sender at 0x7eb628945300>

    async def app(scope: Scope, receive: Receive, send: Send) -> None:
        # Starts customization
        response_awaited = False
        async with AsyncExitStack() as request_stack:
            scope["fastapi_inner_astack"] = request_stack
            async with AsyncExitStack() as function_stack:
                scope["fastapi_function_astack"] = function_stack
>               response = await f(request)
                           ^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/fastapi/routing.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.requests.Request object at 0x7eb62894da00>

    async def app(request: Request) -> Response:
        response: Union[Response, None] = None
        file_stack = request.scope.get("fastapi_middleware_astack")
        assert isinstance(file_stack, AsyncExitStack), (
            "fastapi_middleware_astack not found in request scope"
        )
    
        # Extract endpoint context for error messages
        endpoint_ctx = (
            _extract_endpoint_context(dependant.call)
            if dependant.call
            else EndpointContext()
        )
    
        if dependant.path:
            # For mounted sub-apps, include the mount path prefix
            mount_path = request.scope.get("root_path", "").rstrip("/")
            endpoint_ctx["path"] = f"{request.method} {mount_path}{dependant.path}"
    
        # Read body and auto-close files
        try:
            body: Any = None
            if body_field:
                if is_body_form:
                    body = await request.form()
                    file_stack.push_async_callback(body.close)
                else:
                    body_bytes = await request.body()
                    if body_bytes:
                        json_body: Any = Undefined
                        content_type_value = request.headers.get("content-type")
                        if not content_type_value:
                            json_body = await request.json()
                        else:
                            message = email.message.Message()
                            message["content-type"] = content_type_value
                            if message.get_content_maintype() == "application":
                                subtype = message.get_content_subtype()
                                if subtype == "json" or subtype.endswith("+json"):
                                    json_body = await request.json()
                        if json_body != Undefined:
                            body = json_body
                        else:
                            body = body_bytes
        except json.JSONDecodeError as e:
            validation_error = RequestValidationError(
                [
                    {
                        "type": "json_invalid",
                        "loc": ("body", e.pos),
                        "msg": "JSON decode error",
                        "input": {},
                        "ctx": {"error": e.msg},
                    }
                ],
                body=e.doc,
                endpoint_ctx=endpoint_ctx,
            )
            raise validation_error from e
        except HTTPException:
            # If a middleware raises an HTTPException, it should be raised again
            raise
        except Exception as e:
            http_error = HTTPException(
                status_code=400, detail="There was an error parsing the body"
            )
            raise http_error from e
    
        # Solve dependencies and run path operation function, auto-closing dependencies
        errors: list[Any] = []
        async_exit_stack = request.scope.get("fastapi_inner_astack")
        assert isinstance(async_exit_stack, AsyncExitStack), (
            "fastapi_inner_astack not found in request scope"
        )
        solved_result = await solve_dependencies(
            request=request,
            dependant=dependant,
            body=body,
            dependency_overrides_provider=dependency_overrides_provider,
            async_exit_stack=async_exit_stack,
            embed_body_fields=embed_body_fields,
        )
        errors = solved_result.errors
        if not errors:
>           raw_response = await run_endpoint_function(
                dependant=dependant,
                values=solved_result.values,
                is_coroutine=is_coroutine,
            )

.venv/lib/python3.12/site-packages/fastapi/routing.py:355: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    async def run_endpoint_function(
        *, dependant: Dependant, values: dict[str, Any], is_coroutine: bool
    ) -> Any:
        # Only called by get_request_handler. Has been split into its own function to
        # facilitate profiling endpoints, since inner functions are harder to profile.
        assert dependant.call is not None, "dependant.call must be a function"
    
        if is_coroutine:
>           return await dependant.call(**values)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/fastapi/routing.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = <starlette.requests.Request object at 0x7eb62894da00>
file = UploadFile(filename='test_integration_5fd01fbb.pdf', size=1244, headers=Headers({'content-disposition': 'form-data; name="file"; filename="test_integration_5fd01fbb.pdf"', 'content-type': 'application/pdf'}))
tenant_id = None
session = <sqlalchemy.ext.asyncio.session.AsyncSession object at 0x7eb62894e7b0>

    @router.post(
        "",
        status_code=status.HTTP_202_ACCEPTED,
        response_model=DocumentUploadResponse,
        summary="Upload Document",
        description="""
        Upload a document for ingestion into the knowledge base.
    
        Returns 202 Accepted immediately with a document ID.
        Use the events_url to monitor processing progress via SSE.
        """,
    )
    async def upload_document(
        request: Request,
        file: UploadFile = File(..., description="Document file to upload"),
        tenant_id: str = Form(default=None, description="Tenant ID (optional, super admin only)"),
        session: AsyncSession = Depends(get_db_session),
    ) -> DocumentUploadResponse:
        """
        Upload a document for async ingestion.
        """
        from src.core.ingestion.application.use_cases_documents import UploadDocumentRequest
    
        # Resolve Tenant
        permissions = getattr(request.state, "permissions", [])
        is_super_admin = "super_admin" in permissions
    
        target_tenant_id = None
        if is_super_admin and tenant_id:
            target_tenant_id = tenant_id
        else:
            target_tenant_id = _get_tenant_id(request)
    
        # Read file content
        content = await file.read()
    
        # Build use case with dependencies
        from src.amber_platform.composition_root import build_upload_document_use_case
    
        max_size = settings.uploads.max_size_mb * 1024 * 1024
        use_case = build_upload_document_use_case(session=session, max_size_bytes=max_size)
    
        # Execute use case
        try:
>           result = await use_case.execute(
                UploadDocumentRequest(
                    tenant_id=target_tenant_id,
                    filename=file.filename or "unnamed",
                    content=content,
                    content_type=file.content_type or "application/octet-stream",
                )
            )

src/api/routes/documents.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.ingestion.application.use_cases_documents.UploadDocumentUseCase object at 0x7eb628617800>
request = UploadDocumentRequest(tenant_id='integration_test_tenant', filename='test_integration_5fd01fbb.pdf', content=b'%PDF-1....ze 6\n/Root 1 0 R\n>>\nstartxref\n882\n%%EOF\n\n% Random: 5fd01fbb 1771003242.5259762', content_type='application/pdf')

    async def execute(self, request: UploadDocumentRequest) -> UploadDocumentResult:
        """
        Execute the document upload use case.
    
        Args:
            request: Upload request with tenant, filename, content.
    
        Returns:
            UploadDocumentResult with document_id and status.
    
        Raises:
            ValueError: If file is empty or too large.
        """
        # Validate file size
        if len(request.content) == 0:
            raise ValueError("Empty file uploaded")
    
        if len(request.content) > self._max_size_bytes:
            max_mb = self._max_size_bytes // (1024 * 1024)
            raise ValueError(f"File too large. Max size: {max_mb}MB")
    
        # Register document
        from src.core.ingestion.application.ingestion_service import IngestionService
        from src.core.state.machine import DocumentStatus
    
        service = IngestionService(
            document_repository=self._document_repository,
            tenant_repository=self._tenant_repository,
            unit_of_work=self._unit_of_work,
            storage_client=self._storage,
            neo4j_client=self._graph_client,
            vector_store=self._vector_store,
            vector_store_factory=self._vector_store_factory,
            event_dispatcher=self._event_dispatcher,
        )
        document = await service.register_document(
            tenant_id=request.tenant_id,
            filename=request.filename,
            file_content=request.content,
            content_type=request.content_type,
        )
    
        # Commit transaction before dispatching async processing
        await self._unit_of_work.commit()
    
        # Dispatch async processing if new document
        is_duplicate = document.status != DocumentStatus.INGESTED
        if not is_duplicate:
            if self._task_dispatcher:
>               await self._task_dispatcher.dispatch(
                    "src.workers.tasks.process_document", args=[document.id, request.tenant_id]
                )

src/core/ingestion/application/use_cases_documents.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.infrastructure.adapters.celery_dispatcher.CeleryTaskDispatcher object at 0x7eb62894f440>
task_name = 'src.workers.tasks.process_document'
args = ['doc_313242de275f5512', 'integration_test_tenant'], kwargs = {}

    async def dispatch(
        self, task_name: str, args: list[Any] | None = None, kwargs: dict[str, Any] | None = None
    ) -> str:
        args = args or []
        kwargs = kwargs or {}
    
        try:
            # We assume task_name matches the registered Celery task name string
            # OR we can map specific domain names to celery function names here if we want strict decoupling.
            # For simplicity, we assume strict naming.
    
            # Note: Celery send_task is synchronous unless we use its async API (rarely used).
            # Usually send_task is fast enough (push to Redis).
            # Check for eager execution
            if celery_app.conf.task_always_eager:
                if task_name in celery_app.tasks:
                    task = celery_app.tasks[task_name]
                    result = task.apply(args=args, kwargs=kwargs)
                    return str(result.id)
                else:
                    logger.warning(
                        f"Task {task_name} not found in registry for eager execution. Falling back to send_task."
                    )
    
            result = celery_app.send_task(task_name, args=args, kwargs=kwargs)
            return str(result.id)
        except Exception as e:
            logger.error(f"Failed to dispatch task {task_name}: {e}")
>           raise RuntimeError(f"Task dispatch failed: {e}") from e
E           RuntimeError: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)

src/infrastructure/adapters/celery_dispatcher.py:50: RuntimeError
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2026-02-13 18:20:37,081", "level": "INFO", "message": "Connected to Neo4j at bolt://localhost:7687", "module": "neo4j_client", "function": "connect", "line": 55}
{"timestamp": "2026-02-13 18:20:37,730", "level": "INFO", "message": "Connected to Milvus at localhost:19530", "module": "milvus", "function": "connect", "line": 158}
{"timestamp": "2026-02-13 18:20:38,621", "level": "INFO", "message": "Deleted 0 chunks for tenant integration_test_tenant", "module": "milvus", "function": "delete_by_tenant", "line": 599}
{"timestamp": "2026-02-13 18:20:38,625", "level": "INFO", "message": "Creating collection: amber_integration_test_tenant", "module": "milvus", "function": "_create_collection", "line": 169}
{"timestamp": "2026-02-13 18:20:41,123", "level": "INFO", "message": "Collection amber_integration_test_tenant created with HNSW index and Dynamic Fields", "module": "milvus", "function": "_create_collection", "line": 259}
{"timestamp": "2026-02-13 18:20:41,123", "level": "INFO", "message": "Connected to Milvus at localhost:19530", "module": "milvus", "function": "connect", "line": 158}
{"timestamp": "2026-02-13 18:20:41,127", "level": "WARNING", "message": "Dropped collection amber_integration_test_tenant", "module": "milvus", "function": "drop_collection", "line": 798}
Consider using the pymupdf_layout package for a greatly improved page layout analysis.
------------------------------ Captured log setup ------------------------------
INFO     src.core.graph.infrastructure.neo4j_client:neo4j_client.py:55 Connected to Neo4j at bolt://localhost:7687
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:158 Connected to Milvus at localhost:19530
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:599 Deleted 0 chunks for tenant integration_test_tenant
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:169 Creating collection: amber_integration_test_tenant
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:259 Collection amber_integration_test_tenant created with HNSW index and Dynamic Fields
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:158 Connected to Milvus at localhost:19530
WARNING  src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:798 Dropped collection amber_integration_test_tenant
----------------------------- Captured stdout call -----------------------------

0. Pipeline running under tenant isolation.

1. Uploading document...
{"timestamp": "2026-02-13 18:20:42,673", "level": "INFO", "message": "State Change [Doc: doc_313242de275f5512] None -> ingested", "module": "dispatcher", "function": "emit_state_change", "line": 44, "request_id": "req_f78ac4eee4364a149465f2c315746fed"}
{"timestamp": "2026-02-13 18:20:42,676", "level": "INFO", "message": "Registered new document: test_integration_5fd01fbb.pdf (ID: doc_313242de275f5512)", "module": "ingestion_service", "function": "register_document", "line": 171, "request_id": "req_f78ac4eee4364a149465f2c315746fed"}
{"timestamp": "2026-02-13 18:20:42,683", "level": "INFO", "message": "[Task 8c2b225b-9e41-4807-beb2-9b75309904d8] Starting processing for document doc_313242de275f5512", "module": "tasks", "function": "process_document", "line": 141, "request_id": "req_f78ac4eee4364a149465f2c315746fed"}
{"timestamp": "2026-02-13 18:20:42,684", "level": "INFO", "message": "Executing deep reset of singletons for background task isolation", "module": "tasks", "function": "deep_reset_singletons", "line": 363}
{"timestamp": "2026-02-13 18:20:42,685", "level": "INFO", "message": "Platform registry singletons reset.", "module": "tasks", "function": "deep_reset_singletons", "line": 413}
{"timestamp": "2026-02-13 18:20:42,774", "level": "INFO", "message": "DEBUG: MinIO Fetching integration_test_tenant/doc_313242de275f5512/test_integration_5fd01fbb.pdf from documents", "module": "storage_client", "function": "get_file", "line": 123}
{"timestamp": "2026-02-13 18:20:42,782", "level": "INFO", "message": "Attempting extraction with pymupdf4llm for test_integration_5fd01fbb.pdf", "module": "fallback", "function": "extract_with_fallback", "line": 39}
{"timestamp": "2026-02-13 18:20:42,806", "level": "INFO", "message": "State Change [Doc: doc_313242de275f5512] extracting -> classifying", "module": "dispatcher", "function": "emit_state_change", "line": 44}
{"timestamp": "2026-02-13 18:20:42,806", "level": "WARNING", "message": "Failed to publish event: Task <Task pending name='Task-88' coro=<_process_document_async() running at /home/daniele/Amber_2.0/src/workers/tasks.py:541> cb=[_run_until_complete_cb() at /usr/lib/python3.12/asyncio/base_events.py:182]> got Future <Future pending> attached to a different loop", "module": "dispatcher", "function": "emit_state_change", "line": 67}
{"timestamp": "2026-02-13 18:20:42,808", "level": "INFO", "message": "Classification cache hit for 2405800f3353b3d24bc0f5b3baeec06497ef21ee9b6ef4db2dbfed69f597b8e3", "module": "classifier", "function": "classify", "line": 74}
{"timestamp": "2026-02-13 18:20:42,809", "level": "INFO", "message": "Classified document doc_313242de275f5512 as general. Strategy: general", "module": "ingestion_service", "function": "process_document", "line": 261}
{"timestamp": "2026-02-13 18:20:42,814", "level": "INFO", "message": "State Change [Doc: doc_313242de275f5512] classifying -> chunking", "module": "dispatcher", "function": "emit_state_change", "line": 44}
{"timestamp": "2026-02-13 18:20:42,817", "level": "INFO", "message": "Document doc_313242de275f5512 split into 4 chunks", "module": "ingestion_service", "function": "process_document", "line": 320}
{"timestamp": "2026-02-13 18:20:42,826", "level": "INFO", "message": "State Change [Doc: doc_313242de275f5512] chunking -> embedding", "module": "dispatcher", "function": "emit_state_change", "line": 44}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "RESOLVED EMBEDDING CONFIG | Document: doc_313242de275f5512 | Tenant: integration_test_tenant", "module": "ingestion_service", "function": "process_document", "line": 424}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "  - Tenant Config Provider: None (sys default: ollama)", "module": "ingestion_service", "function": "process_document", "line": 427}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "  - Tenant Config Model: None (sys default: nomic-embed-text)", "module": "ingestion_service", "function": "process_document", "line": 430}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "  - Resolved Provider: ollama", "module": "ingestion_service", "function": "process_document", "line": 433}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "  - Resolved Model: nomic-embed-text", "module": "ingestion_service", "function": "process_document", "line": 434}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "  - Factory: ProviderFactory", "module": "ingestion_service", "function": "process_document", "line": 435}
{"timestamp": "2026-02-13 18:20:42,831", "level": "INFO", "message": "Ollama capacity limiter initialized | enabled=True, total=6, reserved_chat=2, reserved_ingestion=2, redis_url=<class 'set'>", "module": "llm_capacity", "function": "get_ollama_capacity_limiter", "line": 352}
{"timestamp": "2026-02-13 18:20:43,339", "level": "INFO", "message": "Retrying request to /embeddings in 0.440109 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:43,817", "level": "INFO", "message": "Retrying request to /embeddings in 0.957231 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:44,817", "level": "WARNING", "message": "Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.", "module": "embeddings_service", "function": "<lambda>", "line": 219}
{"timestamp": "2026-02-13 18:20:45,859", "level": "INFO", "message": "Retrying request to /embeddings in 0.474179 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:46,370", "level": "INFO", "message": "Retrying request to /embeddings in 0.986697 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:47,395", "level": "WARNING", "message": "Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.", "module": "embeddings_service", "function": "<lambda>", "line": 219}
{"timestamp": "2026-02-13 18:20:49,482", "level": "INFO", "message": "Retrying request to /embeddings in 0.399816 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:49,919", "level": "INFO", "message": "Retrying request to /embeddings in 0.929110 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:50,888", "level": "WARNING", "message": "Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.", "module": "embeddings_service", "function": "<lambda>", "line": 219}
{"timestamp": "2026-02-13 18:20:54,929", "level": "INFO", "message": "Retrying request to /embeddings in 0.492242 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:55,465", "level": "INFO", "message": "Retrying request to /embeddings in 0.964746 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:20:56,470", "level": "WARNING", "message": "Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.", "module": "embeddings_service", "function": "<lambda>", "line": 219}
{"timestamp": "2026-02-13 18:21:04,517", "level": "INFO", "message": "Retrying request to /embeddings in 0.485417 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:21:05,042", "level": "INFO", "message": "Retrying request to /embeddings in 0.869981 seconds", "module": "_base_client", "function": "_sleep_for_retry", "line": 1618}
{"timestamp": "2026-02-13 18:21:05,953", "level": "ERROR", "message": "Embedding generation/storage failed for document doc_313242de275f5512: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]", "module": "ingestion_service", "function": "process_document", "line": 566}
{"timestamp": "2026-02-13 18:21:05,954", "level": "ERROR", "message": "Failed to process document doc_313242de275f5512", "module": "ingestion_service", "function": "process_document", "line": 696, "exception": "Traceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1529, in request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 447, in embed\n    response = await self.client.embeddings.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py\", line 251, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1561, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 231, in _embed_batch_with_retry\n    return await self.provider.embed(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 510, in embed\n    self._handle_error(e, model)\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 531, in _handle_error\n    raise ProviderUnavailableError(\nsrc.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py\", line 465, in process_document\n    embeddings, stats = await embedding_service.embed_texts(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 187, in embed_texts\n    result = await self._embed_batch_with_retry(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]"}
{"timestamp": "2026-02-13 18:21:05,982", "level": "ERROR", "message": "[Task 8c2b225b-9e41-4807-beb2-9b75309904d8] Failed processing document doc_313242de275f5512: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1529, in request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 447, in embed\n    response = await self.client.embeddings.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py\", line 251, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1561, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 231, in _embed_batch_with_retry\n    return await self.provider.embed(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 510, in embed\n    self._handle_error(e, model)\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 531, in _handle_error\n    raise ProviderUnavailableError(\nsrc.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 144, in process_document\n    result = run_async(_process_document_async(document_id, tenant_id, self.request.id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 76, in run_async\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 67, in runner\n    res = asyncio.run(coro)\n          ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 541, in _process_document_async\n    await service.process_document(document_id)\n  File \"/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py\", line 465, in process_document\n    embeddings, stats = await embedding_service.embed_texts(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 187, in embed_texts\n    result = await self._embed_batch_with_retry(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]\n", "module": "tasks", "function": "process_document", "line": 161, "request_id": "req_f78ac4eee4364a149465f2c315746fed"}
{"timestamp": "2026-02-13 18:21:06,050", "level": "ERROR", "message": "Failed to dispatch task src.workers.tasks.process_document: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)", "module": "celery_dispatcher", "function": "dispatch", "line": 49, "request_id": "req_f78ac4eee4364a149465f2c315746fed"}
{"timestamp": "2026-02-13 18:21:06,052", "level": "ERROR", "message": "Request exception: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)", "module": "middleware", "function": "dispatch", "line": 85, "request_id": "req_f78ac4eee4364a149465f2c315746fed", "method": "POST", "path": "/v1/documents", "status_code": 500, "latency_ms": 23522.84}
{"timestamp": "2026-02-13 18:21:06,052", "level": "ERROR", "message": "Unhandled exception: RuntimeError: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)", "module": "exceptions", "function": "unhandled_exception_handler", "line": 123, "request_id": "req_f78ac4eee4364a149465f2c315746fed", "exception": "Traceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n    raise exc from None\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n    response = await connection.handle_async_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n    stream = await self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n    stream = await self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n    return await self._backend.connect_tcp(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1529, in request\n    response = await self._client.send(\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n    response = await self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n    response = await self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n    response = await self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n    response = await transport.handle_async_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 447, in embed\n    response = await self.client.embeddings.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py\", line 251, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1561, in request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 231, in _embed_batch_with_retry\n    return await self.provider.embed(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 510, in embed\n    self._handle_error(e, model)\n  File \"/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py\", line 531, in _handle_error\n    raise ProviderUnavailableError(\nsrc.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 144, in process_document\n    result = run_async(_process_document_async(document_id, tenant_id, self.request.id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 76, in run_async\n    res = future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 67, in runner\n    res = asyncio.run(coro)\n          ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 541, in _process_document_async\n    await service.process_document(document_id)\n  File \"/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py\", line 465, in process_document\n    embeddings, stats = await embedding_service.embed_texts(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py\", line 187, in embed_texts\n    result = await self._embed_batch_with_retry(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/src/infrastructure/adapters/celery_dispatcher.py\", line 39, in dispatch\n    result = task.apply(args=args, kwargs=kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/task.py\", line 843, in apply\n    ret = tracer(task_id, args, kwargs, request)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/trace.py\", line 494, in trace_task\n    I, R, state, retval = on_error(\n                          ^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/trace.py\", line 479, in trace_task\n    R = retval = fun(*args, **kwargs)\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/autoretry.py\", line 38, in run\n    return task._orig_run(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/workers/tasks.py\", line 173, in process_document\n    raise self.retry(exc=e)\n          ^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/task.py\", line 763, in retry\n    raise ret\ncelery.exceptions.Retry: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/middleware/request_id.py\", line 58, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/middleware/timing.py\", line 32, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/admin_ops/infrastructure/observability/middleware.py\", line 96, in dispatch\n    raise e\n  File \"/home/daniele/Amber_2.0/src/core/admin_ops/infrastructure/observability/middleware.py\", line 61, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/middleware/rate_limit.py\", line 135, in dispatch\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/middleware/auth.py\", line 232, in dispatch\n    return await call_next(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 191, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py\", line 85, in collapse_excgroups\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 193, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/middleware/rate_limit.py\", line 187, in dispatch\n    return await call_next(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 168, in call_next\n    raise app_exc from app_exc.__cause__ or app_exc.__context__\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py\", line 144, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 716, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 736, in app\n    await route.handle(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py\", line 290, in handle\n    await self.app(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 115, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 101, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 355, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py\", line 243, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/api/routes/documents.py\", line 151, in upload_document\n    result = await use_case.execute(\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniele/Amber_2.0/src/core/ingestion/application/use_cases_documents.py\", line 154, in execute\n    await self._task_dispatcher.dispatch(\n  File \"/home/daniele/Amber_2.0/src/infrastructure/adapters/celery_dispatcher.py\", line 50, in dispatch\n    raise RuntimeError(f\"Task dispatch failed: {e}\") from e\nRuntimeError: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)"}
------------------------------ Captured log call -------------------------------
INFO     src.core.events.dispatcher:dispatcher.py:44 State Change [Doc: doc_313242de275f5512] None -> ingested
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:171 Registered new document: test_integration_5fd01fbb.pdf (ID: doc_313242de275f5512)
INFO     src.workers.tasks:tasks.py:141 [Task 8c2b225b-9e41-4807-beb2-9b75309904d8] Starting processing for document doc_313242de275f5512
INFO     src.workers.tasks:tasks.py:363 Executing deep reset of singletons for background task isolation
INFO     src.workers.tasks:tasks.py:413 Platform registry singletons reset.
INFO     src.core.ingestion.infrastructure.storage.storage_client:storage_client.py:123 DEBUG: MinIO Fetching integration_test_tenant/doc_313242de275f5512/test_integration_5fd01fbb.pdf from documents
INFO     src.core.ingestion.infrastructure.extraction.fallback:fallback.py:39 Attempting extraction with pymupdf4llm for test_integration_5fd01fbb.pdf
INFO     src.core.events.dispatcher:dispatcher.py:44 State Change [Doc: doc_313242de275f5512] extracting -> classifying
WARNING  src.core.events.dispatcher:dispatcher.py:67 Failed to publish event: Task <Task pending name='Task-88' coro=<_process_document_async() running at /home/daniele/Amber_2.0/src/workers/tasks.py:541> cb=[_run_until_complete_cb() at /usr/lib/python3.12/asyncio/base_events.py:182]> got Future <Future pending> attached to a different loop
INFO     src.core.generation.application.intelligence.classifier:classifier.py:74 Classification cache hit for 2405800f3353b3d24bc0f5b3baeec06497ef21ee9b6ef4db2dbfed69f597b8e3
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:261 Classified document doc_313242de275f5512 as general. Strategy: general
INFO     src.core.events.dispatcher:dispatcher.py:44 State Change [Doc: doc_313242de275f5512] classifying -> chunking
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:320 Document doc_313242de275f5512 split into 4 chunks
INFO     src.core.events.dispatcher:dispatcher.py:44 State Change [Doc: doc_313242de275f5512] chunking -> embedding
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:424 RESOLVED EMBEDDING CONFIG | Document: doc_313242de275f5512 | Tenant: integration_test_tenant
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:427   - Tenant Config Provider: None (sys default: ollama)
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:430   - Tenant Config Model: None (sys default: nomic-embed-text)
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:433   - Resolved Provider: ollama
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:434   - Resolved Model: nomic-embed-text
INFO     src.core.ingestion.application.ingestion_service:ingestion_service.py:435   - Factory: ProviderFactory
INFO     src.shared.llm_capacity:llm_capacity.py:352 Ollama capacity limiter initialized | enabled=True, total=6, reserved_chat=2, reserved_ingestion=2, redis_url=<class 'set'>
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.440109 seconds
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.957231 seconds
WARNING  src.core.retrieval.application.embeddings_service:embeddings_service.py:219 Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.474179 seconds
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.986697 seconds
WARNING  src.core.retrieval.application.embeddings_service:embeddings_service.py:219 Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.399816 seconds
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.929110 seconds
WARNING  src.core.retrieval.application.embeddings_service:embeddings_service.py:219 Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.492242 seconds
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.964746 seconds
WARNING  src.core.retrieval.application.embeddings_service:embeddings_service.py:219 Retrying embedding after [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.485417 seconds
INFO     openai._base_client:_base_client.py:1618 Retrying request to /embeddings in 0.869981 seconds
ERROR    src.core.ingestion.application.ingestion_service:ingestion_service.py:566 Embedding generation/storage failed for document doc_313242de275f5512: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]
ERROR    src.core.ingestion.application.ingestion_service:ingestion_service.py:696 Failed to process document doc_313242de275f5512
Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 101, in handle_async_request
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py", line 113, in connect_tcp
    with map_exceptions(exc_map):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1529, in request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 447, in embed
    response = await self.client.embeddings.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py", line 251, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1561, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    result = await fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 231, in _embed_batch_with_retry
    return await self.provider.embed(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 510, in embed
    self._handle_error(e, model)
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 531, in _handle_error
    raise ProviderUnavailableError(
src.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py", line 465, in process_document
    embeddings, stats = await embedding_service.embed_texts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 187, in embed_texts
    result = await self._embed_batch_with_retry(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 189, in async_wrapped
    return await copy(fn, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    do = await self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 421, in exc_check
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]
ERROR    src.workers.tasks:tasks.py:161 [Task 8c2b225b-9e41-4807-beb2-9b75309904d8] Failed processing document doc_313242de275f5512: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]
Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 101, in handle_async_request
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py", line 113, in connect_tcp
    with map_exceptions(exc_map):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1529, in request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 447, in embed
    response = await self.client.embeddings.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py", line 251, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1561, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    result = await fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 231, in _embed_batch_with_retry
    return await self.provider.embed(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 510, in embed
    self._handle_error(e, model)
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 531, in _handle_error
    raise ProviderUnavailableError(
src.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 144, in process_document
    result = run_async(_process_document_async(document_id, tenant_id, self.request.id))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 76, in run_async
    res = future.result()
          ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/usr/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 67, in runner
    res = asyncio.run(coro)
          ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 541, in _process_document_async
    await service.process_document(document_id)
  File "/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py", line 465, in process_document
    embeddings, stats = await embedding_service.embed_texts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 187, in embed_texts
    result = await self._embed_batch_with_retry(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 189, in async_wrapped
    return await copy(fn, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    do = await self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 421, in exc_check
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]

ERROR    src.infrastructure.adapters.celery_dispatcher:celery_dispatcher.py:49 Failed to dispatch task src.workers.tasks.process_document: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)
ERROR    src.core.admin_ops.infrastructure.observability.middleware:middleware.py:85 Request exception: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)
ERROR    src.api.middleware.exceptions:exceptions.py:123 Unhandled exception: RuntimeError: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)
Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 101, in handle_async_request
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 78, in handle_async_request
    stream = await self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py", line 124, in _connect
    stream = await self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py", line 31, in connect_tcp
    return await self._backend.connect_tcp(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py", line 113, in connect_tcp
    with map_exceptions(exc_map):
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1529, in request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno -2] Name or service not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 447, in embed
    response = await self.client.embeddings.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py", line 251, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1561, in request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 114, in __call__
    result = await fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 231, in _embed_batch_with_retry
    return await self.provider.embed(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 510, in embed
    self._handle_error(e, model)
  File "/home/daniele/Amber_2.0/src/core/generation/infrastructure/providers/ollama.py", line 531, in _handle_error
    raise ProviderUnavailableError(
src.shared.provider_models.ProviderUnavailableError: [ollama] Cannot connect to Ollama at http://host.docker.internal:11434/v1: Connection error.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 144, in process_document
    result = run_async(_process_document_async(document_id, tenant_id, self.request.id))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 76, in run_async
    res = future.result()
          ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/usr/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 67, in runner
    res = asyncio.run(coro)
          ^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 541, in _process_document_async
    await service.process_document(document_id)
  File "/home/daniele/Amber_2.0/src/core/ingestion/application/ingestion_service.py", line 465, in process_document
    embeddings, stats = await embedding_service.embed_texts(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/retrieval/application/embeddings_service.py", line 187, in embed_texts
    result = await self._embed_batch_with_retry(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 189, in async_wrapped
    return await copy(fn, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 111, in __call__
    do = await self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 421, in exc_check
    raise retry_exc from fut.exception()
tenacity.RetryError: RetryError[<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/src/infrastructure/adapters/celery_dispatcher.py", line 39, in dispatch
    result = task.apply(args=args, kwargs=kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/task.py", line 843, in apply
    ret = tracer(task_id, args, kwargs, request)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/trace.py", line 494, in trace_task
    I, R, state, retval = on_error(
                          ^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/trace.py", line 479, in trace_task
    R = retval = fun(*args, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/autoretry.py", line 38, in run
    return task._orig_run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/workers/tasks.py", line 173, in process_document
    raise self.retry(exc=e)
          ^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/celery/app/task.py", line 763, in retry
    raise ret
celery.exceptions.Retry: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/middleware/request_id.py", line 58, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/middleware/timing.py", line 32, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/admin_ops/infrastructure/observability/middleware.py", line 96, in dispatch
    raise e
  File "/home/daniele/Amber_2.0/src/core/admin_ops/infrastructure/observability/middleware.py", line 61, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/middleware/rate_limit.py", line 135, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/middleware/auth.py", line 232, in dispatch
    return await call_next(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 191, in __call__
    with recv_stream, send_stream, collapse_excgroups():
  File "/usr/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 85, in collapse_excgroups
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/middleware/rate_limit.py", line 187, in dispatch
    return await call_next(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 168, in call_next
    raise app_exc from app_exc.__cause__ or app_exc.__context__
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 144, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/api/routes/documents.py", line 151, in upload_document
    result = await use_case.execute(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daniele/Amber_2.0/src/core/ingestion/application/use_cases_documents.py", line 154, in execute
    await self._task_dispatcher.dispatch(
  File "/home/daniele/Amber_2.0/src/infrastructure/adapters/celery_dispatcher.py", line 50, in dispatch
    raise RuntimeError(f"Task dispatch failed: {e}") from e
RuntimeError: Task dispatch failed: Retry in 60s: RetryError(<Future at 0x7eb5fa97fad0 state=finished raised ProviderUnavailableError>)
--------------------------- Captured stdout teardown ---------------------------
{"timestamp": "2026-02-13 18:21:06,707", "level": "INFO", "message": "Connected to Neo4j at bolt://localhost:7687", "module": "neo4j_client", "function": "connect", "line": 55}
{"timestamp": "2026-02-13 18:21:07,146", "level": "INFO", "message": "Connected to Milvus at localhost:19530", "module": "milvus", "function": "connect", "line": 158}
{"timestamp": "2026-02-13 18:21:08,219", "level": "INFO", "message": "Deleted 0 chunks for tenant integration_test_tenant", "module": "milvus", "function": "delete_by_tenant", "line": 599}
{"timestamp": "2026-02-13 18:21:08,223", "level": "INFO", "message": "Creating collection: amber_integration_test_tenant", "module": "milvus", "function": "_create_collection", "line": 169}
{"timestamp": "2026-02-13 18:21:10,727", "level": "INFO", "message": "Collection amber_integration_test_tenant created with HNSW index and Dynamic Fields", "module": "milvus", "function": "_create_collection", "line": 259}
{"timestamp": "2026-02-13 18:21:10,727", "level": "INFO", "message": "Connected to Milvus at localhost:19530", "module": "milvus", "function": "connect", "line": 158}
{"timestamp": "2026-02-13 18:21:10,735", "level": "WARNING", "message": "Dropped collection amber_integration_test_tenant", "module": "milvus", "function": "drop_collection", "line": 798}
---------------------------- Captured log teardown -----------------------------
INFO     src.core.graph.infrastructure.neo4j_client:neo4j_client.py:55 Connected to Neo4j at bolt://localhost:7687
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:158 Connected to Milvus at localhost:19530
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:599 Deleted 0 chunks for tenant integration_test_tenant
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:169 Creating collection: amber_integration_test_tenant
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:259 Collection amber_integration_test_tenant created with HNSW index and Dynamic Fields
INFO     src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:158 Connected to Milvus at localhost:19530
WARNING  src.core.retrieval.infrastructure.vector_store.milvus:milvus.py:798 Dropped collection amber_integration_test_tenant
=========================== short test summary info ============================
FAILED tests/integration/test_ingestion_pipeline.py::TestIngestionPipeline::test_complete_pipeline
============================== 1 failed in 33.99s ==============================

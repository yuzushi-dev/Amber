"""
Document API Routes
===================

Endpoints for document management.
Phase 1: Full implementation with async processing.
"""

import io
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any

from fastapi import APIRouter, UploadFile, File, Form, HTTPException, status, Depends, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from src.api.deps import get_db_session
from src.api.config import settings
from src.core.models.document import Document
from src.core.state.machine import DocumentStatus
from src.core.storage.minio_client import MinIOClient
from src.core.services.ingestion import IngestionService
from src.core.graph.neo4j_client import neo4j_client
from sse_starlette.sse import EventSourceResponse
import redis.asyncio as redis
import asyncio
import json

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents", tags=["documents"])


def _get_content_type(document: Document) -> Optional[str]:
    """
    Get content type for a document.

    Checks metadata first, then derives from filename extension.
    """
    # Check if content_type is stored in metadata
    if document.metadata_ and isinstance(document.metadata_, dict):
        content_type = document.metadata_.get("content_type")
        if content_type:
            return content_type

    # Derive from filename extension
    filename = document.filename.lower()
    if filename.endswith('.pdf'):
        return 'application/pdf'
    elif filename.endswith(('.md', '.markdown')):
        return 'text/markdown'
    elif filename.endswith('.txt'):
        return 'text/plain'
    elif filename.endswith('.html'):
        return 'text/html'
    elif filename.endswith('.json'):
        return 'application/json'
    elif filename.endswith('.csv'):
        return 'text/csv'
    else:
        return 'text/plain'  # Default fallback


def _get_tenant_id(request: Request) -> str:
    """Resolve tenant ID from request context or default settings."""
    if hasattr(request.state, "tenant_id"):
        return str(request.state.tenant_id)
    return settings.tenant_id


class DocumentUploadResponse(BaseModel):
    """Response model for document upload."""
    document_id: str
    status: str
    events_url: str
    message: str


class DocumentResponse(BaseModel):
    """Response model for document details."""
    id: str
    filename: str
    title: str  # Alias for filename (for frontend compatibility)
    status: str
    domain: Optional[str] = None
    tenant_id: str
    source_type: Optional[str] = "upload"
    content_type: Optional[str] = None  # MIME type of the document
    created_at: datetime


@router.post(
    "",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=DocumentUploadResponse,
    summary="Upload Document",
    description="""
    Upload a document for ingestion into the knowledge base.
    
    Returns 202 Accepted immediately with a document ID.
    Use the events_url to monitor processing progress via SSE.
    """,
)
async def upload_document(
    file: UploadFile = File(..., description="Document file to upload"),
    tenant_id: str = Form(default=None, description="Tenant ID (optional, uses default)"),
    session: AsyncSession = Depends(get_db_session),
) -> DocumentUploadResponse:
    """
    Upload a document for async ingestion.
    """
    # Use default tenant if not provided
    tenant = tenant_id or settings.tenant_id
    
    # Read file content
    content = await file.read()
    
    if len(content) == 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Empty file uploaded"
        )
    
    # Check file size
    max_size = settings.uploads.max_size_mb * 1024 * 1024
    if len(content) > max_size:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail=f"File too large. Max size: {settings.uploads.max_size_mb}MB"
        )
    
    # Register document
    storage = MinIOClient()
    service = IngestionService(session, storage)
    
    document = await service.register_document(
        tenant_id=tenant,
        filename=file.filename or "unnamed",
        file_content=content,
        content_type=file.content_type or "application/octet-stream"
    )
    
    # Dispatch async processing task
    # Fix: Only dispatch if this is a new document to avoid duplicate processing
    if document.status == DocumentStatus.INGESTED:
        from src.workers.tasks import process_document
        process_document.delay(document.id, tenant)
        message = "Document accepted for processing"
    else:
        # Document was deduplicated (existing), so don't re-process
        message = f"Document already exists with status: {document.status.value}"
    
    # Build events URL
    events_url = f"/v1/documents/{document.id}/events"
    
    logger.info(f"Document {document.id} uploaded, processing dispatched")
    
    return DocumentUploadResponse(
        document_id=document.id,
        status=document.status.value,
        events_url=events_url,
        message="Document accepted for processing"
    )


@router.get(
    "/{document_id}/events",
    summary="Document Processing Events",
    description="""
    Server-Sent Events (SSE) endpoint for monitoring document processing status in real-time.

    Subscribe to this endpoint to receive status updates as the document moves through
    the ingestion pipeline (extracting, classifying, chunking, embedding, graph_sync, ready).
    """,
)
async def document_events(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Stream document processing events via SSE.

    This endpoint subscribes to Redis pub/sub for real-time status updates.
    """
    # Verify document exists and get tenant_id
    tenant_id = _get_tenant_id(http_request)
    query = select(Document).where(
        Document.id == document_id,
        Document.tenant_id == tenant_id,
    )
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    async def event_generator():
        """Generate SSE events from Redis pub/sub."""
        redis_client = None
        pubsub = None

        try:
            # Connect to Redis
            redis_client = redis.from_url(settings.db.redis_url, decode_responses=True)
            pubsub = redis_client.pubsub()

            # Subscribe to document status channel
            channel = f"document:{document_id}:status"
            await pubsub.subscribe(channel)

            logger.info(f"SSE client connected for document {document_id}")

            # Send initial status
            yield {
                "event": "status",
                "data": json.dumps({
                    "document_id": document_id,
                    "status": document.status.value,
                    "message": f"Monitoring document {document_id}"
                })
            }

            # Listen for Redis pub/sub messages
            while True:
                try:
                    # Use timeout to allow periodic checks
                    message = await asyncio.wait_for(
                        pubsub.get_message(ignore_subscribe_messages=True),
                        timeout=1.0
                    )

                    if message and message['type'] == 'message':
                        # Forward the Redis message as SSE event
                        data = message['data']

                        # Parse and re-serialize to ensure valid JSON
                        if isinstance(data, str):
                            event_data = json.loads(data)
                        else:
                            event_data = data

                        yield {
                            "event": "status",
                            "data": json.dumps(event_data)
                        }

                        # Close connection if document reached terminal state
                        if event_data.get('status') in ['ready', 'failed', 'completed']:
                            logger.info(f"Document {document_id} reached terminal state: {event_data.get('status')}")
                            break

                except asyncio.TimeoutError:
                    # Send keepalive comment to prevent connection timeout
                    yield {
                        "comment": "keepalive"
                    }
                    continue

        except Exception as e:
            logger.error(f"SSE error for document {document_id}: {e}")
            yield {
                "event": "error",
                "data": json.dumps({"error": str(e)})
            }
        finally:
            # Cleanup
            if pubsub:
                await pubsub.unsubscribe(channel)
                await pubsub.close()
            if redis_client:
                await redis_client.close()
            logger.info(f"SSE client disconnected for document {document_id}")

    return EventSourceResponse(event_generator())


@router.get(
    "",
    summary="List Documents",
    description="List all documents in the knowledge base.",
)
async def list_documents(
    tenant_id: str = None,
    limit: int = 50,
    offset: int = 0,
    session: AsyncSession = Depends(get_db_session),
) -> list[DocumentResponse]:
    """
    List documents in the knowledge base.
    """
    tenant = tenant_id or settings.tenant_id
    
    query = select(Document).where(
        Document.tenant_id == tenant
    ).limit(limit).offset(offset)
    
    result = await session.execute(query)
    documents = result.scalars().all()
    
    return [
        DocumentResponse(
            id=doc.id,
            filename=doc.filename,
            title=doc.filename,  # Alias for frontend
            status=doc.status.value,
            domain=doc.domain,
            tenant_id=doc.tenant_id,
            source_type=doc.source_type,
            content_type=_get_content_type(doc),
            created_at=doc.created_at
        )
        for doc in documents
    ]


@router.get(
    "/{document_id}",
    summary="Get Document",
    description="Get details of a specific document.",
)
async def get_document(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
) -> DocumentResponse:
    """
    Get document details.
    """
    tenant_id = _get_tenant_id(http_request)
    query = select(Document).where(
        Document.id == document_id,
        Document.tenant_id == tenant_id,
    )
    result = await session.execute(query)
    document = result.scalars().first()
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )
    
    return DocumentResponse(
        id=document.id,
        filename=document.filename,
        title=document.filename,  # Alias for frontend
        status=document.status.value,
        domain=document.domain,
        tenant_id=document.tenant_id,
        source_type=document.source_type,
        content_type=_get_content_type(document),
        created_at=document.created_at
    )


@router.get(
    "/{document_id}/file",
    summary="Get Document File",
    description="Download the original document file from storage.",
)
async def get_document_file(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Retrieve the original document file from MinIO storage.

    Returns a streaming response with the file content and appropriate content-type header.
    """
    # Get document metadata
    tenant_id = _get_tenant_id(http_request)
    query = select(Document).where(
        Document.id == document_id,
        Document.tenant_id == tenant_id,
    )
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # Get file from MinIO
    try:
        storage = MinIOClient()
        # Get the raw stream from MinIO (urllib3 response)
        file_stream = storage.get_file_stream(document.storage_path)

        # Determine content type
        content_type = _get_content_type(document)

        # Stream the file back to the client
        # We pass the stream directly to StreamingResponse
        return StreamingResponse(
            file_stream,
            media_type=content_type,
            headers={
                "Content-Disposition": f'inline; filename="{document.filename}"'
            }
        )

    except Exception as e:
        logger.error(f"Failed to retrieve file for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve file: {str(e)}"
        )


@router.delete(
    "/{document_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete Document",
    description="Delete a document from the knowledge base.",
)
async def delete_document(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Delete a document.
    """
    tenant_id = _get_tenant_id(http_request)
    query = select(Document).where(
        Document.id == document_id,
        Document.tenant_id == tenant_id,
    )
    result = await session.execute(query)
    document = result.scalars().first()
    
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )
    
    # Delete from Neo4j graph
    try:
        cypher = """
        MATCH (d:Document {id: $document_id, tenant_id: $tenant_id})
        OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)
        OPTIONAL MATCH (c)-[:MENTIONS]->(e:Entity)
        WITH d, c, collect(DISTINCT e) AS entities
        DETACH DELETE d, c
        WITH entities
        UNWIND entities AS entity
        WHERE entity IS NOT NULL AND NOT (entity)<-[:MENTIONS]-()
        DETACH DELETE entity
        """
        await neo4j_client.execute_write(
            cypher,
            {"document_id": document_id, "tenant_id": document.tenant_id},
        )
    except Exception as e:
        logger.warning(f"Failed to delete graph data for document {document_id}: {e}")

    # Delete from Milvus vector store
    try:
        from src.core.vector_store.milvus import MilvusVectorStore, MilvusConfig

        milvus_config = MilvusConfig(
            host=settings.db.milvus_host,
            port=settings.db.milvus_port,
            collection_name=f"amber_{document.tenant_id}",
        )
        vector_store = MilvusVectorStore(milvus_config)
        try:
            await vector_store.delete_by_document(document_id, document.tenant_id)
        finally:
            await vector_store.disconnect()
    except Exception as e:
        logger.warning(f"Failed to delete vectors for document {document_id}: {e}")

    # Delete from MinIO
    try:
        storage = MinIOClient()
        storage.delete_file(document.storage_path)
    except Exception as e:
        logger.warning(f"Failed to delete file from storage: {e}")
    
    # Delete from DB (cascades to chunks)
    await session.delete(document)
    await session.commit()
    
    logger.info(f"Document {document_id} deleted")


@router.get(
    "/{document_id}/entities",
    summary="Get Document Entities",
    description="Get entities extracted from a specific document with pagination.",
)
async def get_document_entities(
    document_id: str,
    limit: int = 100,
    offset: int = 0,
    session: AsyncSession = Depends(get_db_session),
) -> List[Dict[str, Any]]:
    """
    Get entities extracted from a specific document via Neo4j.

    Args:
        document_id: Document UUID
        limit: Maximum number of entities to return (default: 100)
        offset: Number of entities to skip (default: 0)
    """
    # 1. Verify existence in SQL and get tenant_id
    query = select(Document).where(Document.id == document_id)
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # 2. Query Neo4j with pagination
    cypher = """
        MATCH (d:Document {id: $document_id, tenant_id: $tenant_id})-[:HAS_CHUNK]->(c:Chunk)
        MATCH (c)-[:MENTIONS]->(e:Entity)
        RETURN DISTINCT e
        ORDER BY e.name
        SKIP $offset
        LIMIT $limit
    """

    try:
        records = await neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id,
                "tenant_id": document.tenant_id,
                "limit": limit,
                "offset": offset
            }
        )
        # Neo4j Node objects can be converted to dict, but driver returns distinct e as Node.
        # We need to extract properties.
        return [dict(record["e"]) for record in records]
    except Exception as e:
        logger.error(f"Failed to fetch entities for document {document_id}: {e}")
        return []


@router.get(
    "/{document_id}/relationships",
    summary="Get Document Relationships",
    description="Get relationships between entities in this document with pagination.",
)
async def get_document_relationships(
    document_id: str,
    limit: int = 100,
    offset: int = 0,
    session: AsyncSession = Depends(get_db_session),
) -> List[Dict[str, Any]]:
    """
    Get relationships between entities in this document via Neo4j.

    Optimized query that avoids Cartesian products.

    Args:
        document_id: Document UUID
        limit: Maximum number of relationships to return (default: 100)
        offset: Number of relationships to skip (default: 0)
    """
    # 1. Verify existence
    query = select(Document).where(Document.id == document_id)
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # 2. Query Neo4j
    # OPTIMIZED: Direct MATCH pattern instead of UNWIND Cartesian product
    # This is O(N) instead of O(NÂ²) where N is the number of entities
    cypher = """
        MATCH (d:Document {id: $document_id, tenant_id: $tenant_id})-[:HAS_CHUNK]->(c:Chunk)
        MATCH (c)-[:MENTIONS]->(s:Entity)
        MATCH (s)-[r:RELATED_TO]->(t:Entity)
        WHERE t.tenant_id = $tenant_id
          AND EXISTS {
              MATCH (d)-[:HAS_CHUNK]->(c2:Chunk)-[:MENTIONS]->(t)
          }
        RETURN DISTINCT {
            source: s.name,
            target: t.name,
            type: r.type,
            description: r.description,
            weight: r.weight
        } as rel
        ORDER BY rel.weight DESC
        SKIP $offset
        LIMIT $limit
    """

    try:
        records = await neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id,
                "tenant_id": document.tenant_id,
                "limit": limit,
                "offset": offset
            }
        )
        return [record["rel"] for record in records]
    except Exception as e:
        logger.error(f"Failed to fetch relationships for document {document_id}: {e}")
        return []


from src.core.models.chunk import Chunk

@router.get(
    "/{document_id}/chunks",
    summary="Get Document Chunks",
    description="Get chunks for a specific document.",
)
async def get_document_chunks(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
) -> List[Dict[str, Any]]:
    """
    Get chunks for a document from PostgreSQL.
    """
    tenant_id = _get_tenant_id(http_request)
    
    # 1. Verify existence
    # We can join with chunks directly or check doc first.
    # Checking doc first gives better error message.
    query = select(Document).where(Document.id == document_id)
    result = await session.execute(query)
    doc = result.scalars().first()
    
    if not doc:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )
    
    # 2. Fetch chunks from Postgres
    chunks_query = (
        select(Chunk)
        .where(Chunk.document_id == document_id)
        .order_by(Chunk.index.asc())
    )
    result = await session.execute(chunks_query)
    chunks = result.scalars().all()
    
    return [
        {
            "id": chunk.id,
            "index": chunk.index,
            "content": chunk.content,
            "tokens": chunk.tokens,
            "embedding_status": chunk.embedding_status.value
        }
        for chunk in chunks
    ]

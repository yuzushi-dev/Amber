"""
Document API Routes
===================

Endpoints for document management.
Phase 1: Full implementation with async processing.
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Any

import redis.asyncio as redis
from fastapi import APIRouter, Depends, File, Form, HTTPException, Request, UploadFile, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sse_starlette.sse import EventSourceResponse

from src.api.config import settings
from src.api.deps import get_db_session as get_db_session
from src.amber_platform.composition_root import platform, build_vector_store_factory
from src.core.ingestion.domain.document import Document
from src.core.ingestion.application.ingestion_service import IngestionService
from src.core.state.machine import DocumentStatus

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents", tags=["documents"])


def _get_content_type(document: Document) -> str | None:
    """
    Get content type for a document.

    Checks metadata first, then derives from filename extension.
    """
    # Check if content_type is stored in metadata
    if document.metadata_ and isinstance(document.metadata_, dict):
        content_type = document.metadata_.get("content_type")
        if content_type:
            return content_type

    # Derive from filename extension
    filename = document.filename.lower()
    if filename.endswith('.pdf'):
        return 'application/pdf'
    elif filename.endswith(('.md', '.markdown')):
        return 'text/markdown'
    elif filename.endswith('.txt'):
        return 'text/plain'
    elif filename.endswith('.html'):
        return 'text/html'
    elif filename.endswith('.json'):
        return 'application/json'
    elif filename.endswith('.csv'):
        return 'text/csv'
    else:
        return 'text/plain'  # Default fallback


def _get_tenant_id(request: Request) -> str:
    """Resolve tenant ID from request context or default settings."""
    if hasattr(request.state, "tenant_id"):
        return str(request.state.tenant_id)
    return settings.tenant_id


class DocumentUploadResponse(BaseModel):
    """Response model for document upload."""
    document_id: str
    status: str
    events_url: str
    message: str


class DocumentResponse(BaseModel):
    """Response model for document details."""
    id: str
    filename: str
    title: str  # Alias for filename (for frontend compatibility)
    status: str
    domain: str | None = None
    tenant_id: str
    folder_id: str | None = None
    source_type: str | None = "upload"
    content_type: str | None = None  # MIME type of the document
    created_at: datetime

    # Enrichment fields
    summary: str | None = None
    document_type: str | None = None
    keywords: list[str] = []
    hashtags: list[str] = []
    metadata: dict[str, Any] | None = None

    # Stats (computed from chunks/entities/relationships)
    stats: dict[str, int] | None = None
    ingestion_cost: float | None = 0.0


@router.post(
    "",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=DocumentUploadResponse,
    summary="Upload Document",
    description="""
    Upload a document for ingestion into the knowledge base.

    Returns 202 Accepted immediately with a document ID.
    Use the events_url to monitor processing progress via SSE.
    """,
)
async def upload_document(
    request: Request,
    file: UploadFile = File(..., description="Document file to upload"),
    tenant_id: str = Form(default=None, description="Tenant ID (optional, super admin only)"),
    session: AsyncSession = Depends(get_db_session),
) -> DocumentUploadResponse:
    """
    Upload a document for async ingestion.
    """
    from src.core.ingestion.application.use_cases_documents import UploadDocumentRequest
    
    # Resolve Tenant
    permissions = getattr(request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions
    
    target_tenant_id = None
    if is_super_admin and tenant_id:
        target_tenant_id = tenant_id
    else:
        target_tenant_id = _get_tenant_id(request)

    # Read file content
    content = await file.read()
    
    # Build use case with dependencies
    from src.amber_platform.composition_root import build_upload_document_use_case

    max_size = settings.uploads.max_size_mb * 1024 * 1024
    use_case = build_upload_document_use_case(session=session, max_size_bytes=max_size)
    
    # Execute use case
    try:
        result = await use_case.execute(
            UploadDocumentRequest(
                tenant_id=target_tenant_id,
                filename=file.filename or "unnamed",
                content=content,
                content_type=file.content_type or "application/octet-stream",
            )
        )
    except ValueError as e:
        # Map domain errors to HTTP errors
        if "empty" in str(e).lower():
            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
        elif "too large" in str(e).lower():
            raise HTTPException(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE, detail=str(e))
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    
    # Build events URL
    events_url = f"/v1/documents/{result.document_id}/events"
    
    logger.info(f"Document {result.document_id} uploaded, processing dispatched")
    
    return DocumentUploadResponse(
        document_id=result.document_id,
        status=result.status,
        events_url=events_url,
        message=result.message,
    )


@router.get(
    "/{document_id}/events",
    summary="Document Processing Events",
    description="""
    Server-Sent Events (SSE) endpoint for monitoring document processing status in real-time.

    Subscribe to this endpoint to receive status updates as the document moves through
    the ingestion pipeline (extracting, classifying, chunking, embedding, graph_sync, ready).
    """,
)
async def document_events(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Stream document processing events via SSE.

    This endpoint subscribes to Redis pub/sub for real-time status updates.
    """
    # Verify document exists and get tenant_id
    # Verify document exists and get tenant_id
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)
    
    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)
        query = query.where(Document.tenant_id == tenant_id)
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    async def event_generator():
        """Generate SSE events from Redis pub/sub."""
        redis_client = None
        pubsub = None

        try:
            # Connect to Redis
            redis_client = redis.from_url(settings.db.redis_url, decode_responses=True)
            pubsub = redis_client.pubsub()

            # Subscribe to document status channel
            channel = f"document:{document_id}:status"
            await pubsub.subscribe(channel)

            logger.info(f"SSE client connected for document {document_id}")

            # Send initial status
            yield {
                "event": "status",
                "data": json.dumps({
                    "document_id": document_id,
                    "status": document.status.value,
                    "message": f"Monitoring document {document_id}"
                })
            }

            # Listen for Redis pub/sub messages
            while True:
                try:
                    # Use timeout to allow periodic checks
                    message = await asyncio.wait_for(
                        pubsub.get_message(ignore_subscribe_messages=True),
                        timeout=1.0
                    )

                    if message and message['type'] == 'message':
                        # Forward the Redis message as SSE event
                        data = message['data']

                        # Parse and re-serialize to ensure valid JSON
                        if isinstance(data, str):
                            event_data = json.loads(data)
                        else:
                            event_data = data

                        yield {
                            "event": "status",
                            "data": json.dumps(event_data)
                        }

                        # Close connection if document reached terminal state
                        if event_data.get('status') in ['ready', 'failed', 'completed']:
                            logger.info(f"Document {document_id} reached terminal state: {event_data.get('status')}")
                            break

                except TimeoutError:
                    # Send keepalive comment to prevent connection timeout
                    yield {
                        "comment": "keepalive"
                    }
                    continue

        except Exception as e:
            logger.error(f"SSE error for document {document_id}: {e}")
            yield {
                "event": "error",
                "data": json.dumps({"error": str(e)})
            }
        finally:
            # Cleanup
            if pubsub:
                await pubsub.unsubscribe(channel)
                await pubsub.close()
            if redis_client:
                await redis_client.close()
            logger.info(f"SSE client disconnected for document {document_id}")

    return EventSourceResponse(event_generator())


@router.get(
    "",
    summary="List Documents",
    description="List all documents in the knowledge base.",
)
async def list_documents(
    http_request: Request,
    tenant_id: str = None,
    limit: int = 50,
    offset: int = 0,
    session: AsyncSession = Depends(get_db_session),
) -> list[DocumentResponse]:
    """
    List documents in the knowledge base.
    """
    # Check permissions
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    query = select(Document)

    if is_super_admin:
        # Super Admin: Show all if no tenant specified
        if tenant_id:
            query = query.where(Document.tenant_id == tenant_id)
    else:
        # Regular User: Enforce current tenant
        # Use tenant from request state (set by auth middleware) or fallback to settings
        current_tenant = getattr(http_request.state, "tenant_id", settings.tenant_id)
        query = query.where(Document.tenant_id == str(current_tenant))

    # Apply pagination
    query = query.limit(limit).offset(offset)

    result = await session.execute(query)
    documents = result.scalars().all()

    return [
        DocumentResponse(
            id=doc.id,
            filename=doc.filename,
            title=doc.filename,  # Alias for frontend
            status=doc.status.value,
            domain=doc.domain,
            tenant_id=doc.tenant_id,
            folder_id=doc.folder_id,
            source_type=doc.source_type,
            content_type=_get_content_type(doc),
            created_at=doc.created_at,
            ingestion_cost=0.0,
        )
        for doc in documents
    ]


@router.get(
    "/{document_id}",
    summary="Get Document",
    description="Get details of a specific document including enrichment data and stats.",
)
async def get_document(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
) -> DocumentResponse:
    """
    Get document details with enrichment data and statistics.
    """
    from src.core.ingestion.application.use_cases_documents import GetDocumentUseCase, GetDocumentRequest, DocumentOutput

    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions
    tenant_id = None
    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)

    use_case = GetDocumentUseCase(session=session, graph_client=platform.neo4j_client)
    try:
        output: DocumentOutput = await use_case.execute(
            GetDocumentRequest(
                document_id=document_id,
                tenant_id=tenant_id,
                is_super_admin=is_super_admin
            )
        )
    except LookupError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e)
        )
    
    return DocumentResponse(
        id=output.id,
        filename=output.filename,
        title=output.title,
        status=output.status,
        domain=output.domain,
        tenant_id=output.tenant_id,
        folder_id=output.folder_id,
        source_type=output.source_type,
        content_type=output.content_type,
        created_at=output.created_at,
        summary=output.summary,
        document_type=output.document_type,
        keywords=output.keywords,
        hashtags=output.hashtags,
        metadata=output.metadata,
        stats=output.stats,
        ingestion_cost=output.ingestion_cost,
    )


@router.get(
    "/{document_id}/communities",
    summary="Get Document Communities",
    description="Get communities (entity clusters) associated with this document.",
)
async def get_document_communities(
    document_id: str,
    limit: int = 50,
    offset: int = 0,
    http_request: Request = None,
    session: AsyncSession = Depends(get_db_session),
) -> list[dict[str, Any]]:
    """
    Get communities (entity clusters) for a document from Neo4j.

    Returns communities with their entities, sorted by entity count.
    """
    # Verify document exists
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
         # Need tenant_id to verify ownership if not super admin
         # We can't easily get it here without request object properly 
         # But http_request is optional in signature? No, let's look at signature.
         # http_request: Request = None 
         # Wait, if it's None we can't check permissions. 
         # But Depends(verify_admin) or auth middleware ensures request is populated?
         # Actually get_document_communities has `http_request: Request = None` ???
         # Checking signature: `http_request: Request = None`. 
         # If called from API, FastAPI injects it? No, Request must be declared.
         # If it's None, we might fail. 
         # But let's assume it's injected if requested.
         if http_request:
             tenant_id = _get_tenant_id(http_request)
             query = query.where(Document.tenant_id == tenant_id)
         else:
             # Fallback or error? If we are here, we probably have a request context.
             # The signature seems to imply it might be optional, but for a router endpoint it should be there.
             # Let's trust `_get_tenant_id` handles request attribute access, but we need request.
             pass
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # Query Neo4j for communities via BELONGS_TO relationship
    cypher = """
        MATCH (d:Document {id: $document_id})-[:HAS_CHUNK]->(c:Chunk)
        MATCH (c)-[:MENTIONS]->(e:Entity)-[:BELONGS_TO]->(comm:Community)
        WITH comm, collect(DISTINCT {
            name: e.name,
            type: e.type,
            description: e.description
        }) AS entities
        RETURN comm.id AS community_id, comm.title AS title, comm.summary AS summary,
               comm.level AS level, entities, size(entities) AS entity_count
        ORDER BY entity_count DESC
        SKIP $offset
        LIMIT $limit
    """

    try:
        records = await platform.neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id,
                "offset": offset,
                "limit": limit,
            }
        )

        return [
            {
                "community_id": record.get("community_id"),
                "title": record.get("title"),
                "summary": record.get("summary"),
                "level": record.get("level"),
                "entity_count": record.get("entity_count", 0),
                "entities": record.get("entities", []),
            }
            for record in records
        ]
    except Exception as e:
        logger.warning(f"Failed to fetch communities for document {document_id}: {e}")
        return []


@router.get(
    "/{document_id}/file",
    summary="Get Document File",
    description="Download the original document file from storage.",
)
async def get_document_file(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Retrieve the original document file from MinIO storage.

    Returns a streaming response with the file content and appropriate content-type header.
    """
    # Get document metadata
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)
        query = query.where(Document.tenant_id == tenant_id)
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # Get file from MinIO
    try:
        storage = platform.minio_client
        # Get the raw stream from MinIO (urllib3 response)
        file_stream = storage.get_file_stream(document.storage_path)

        # Determine content type
        content_type = _get_content_type(document)

        # Stream the file back to the client
        # We pass the stream directly to StreamingResponse
        return StreamingResponse(
            file_stream,
            media_type=content_type,
            headers={
                "Content-Disposition": f'attachment; filename="{document.filename}"',
                "X-Content-Type-Options": "nosniff"
            }
        )

    except Exception as e:
        logger.error(f"Failed to retrieve file for document {document_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve file: {str(e)}"
        ) from e


class DocumentUpdate(BaseModel):
    """Schema for updating a document."""
    title: str | None = None
    folder_id: str | None = None


@router.patch(
    "/{document_id}",
    response_model=DocumentResponse,
    summary="Update Document",
    description="Update document details (e.g., title, folder).",
)
async def update_document(
    document_id: str,
    update_data: DocumentUpdate,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
) -> DocumentResponse:
    """
    Update a document.
    """
    from src.core.ingestion.application.use_cases_documents import UpdateDocumentUseCase, UpdateDocumentRequest, DocumentOutput

    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions
    tenant_id = None
    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)

    use_case = UpdateDocumentUseCase(session=session, graph_client=platform.neo4j_client)
    try:
        output: DocumentOutput = await use_case.execute(
            UpdateDocumentRequest(
                document_id=document_id,
                tenant_id=tenant_id,
                is_super_admin=is_super_admin,
                title=update_data.title,
                folder_id=update_data.folder_id
            )
        )
    except LookupError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e)
        )
    
    return DocumentResponse(
        id=output.id,
        filename=output.filename,
        title=output.title,
        status=output.status,
        domain=output.domain,
        tenant_id=output.tenant_id,
        folder_id=output.folder_id,
        source_type=output.source_type,
        content_type=output.content_type,
        created_at=output.created_at,
        summary=output.summary,
        document_type=output.document_type,
        keywords=output.keywords,
        hashtags=output.hashtags,
        metadata=output.metadata,
        stats=output.stats,
        ingestion_cost=output.ingestion_cost,
    )


@router.delete(
    "/{document_id}",
    status_code=status.HTTP_204_NO_CONTENT,
    summary="Delete Document",
    description="Delete a document from the knowledge base.",
)
async def delete_document(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
):
    """
    Delete a document.
    """
    from src.core.ingestion.application.use_cases_documents import DeleteDocumentRequest, DeleteDocumentUseCase
    
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions
    
    # 1. Resolve Tenant
    tenant_id = None
    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)
    else:
        # If super admin, we might need tenant_id? 
        # The use case finds document by ID. If super admin, ignores tenant_id.
        # But we still need to pass something for strict typing if expected.
        # Use Case handles it.
        tenant_id = "super_admin_context" 

    # 2. Build Dependencies
    # 2. Build Dependencies
    
    vector_store_factory = build_vector_store_factory()
    dimensions = settings.embedding_dimensions or 1536

    def make_vector_store(tid: str):
        return vector_store_factory(dimensions, collection_name=f"amber_{tid}")
    
    use_case = DeleteDocumentUseCase(
        session=session,
        storage=platform.minio_client,
        graph_client=platform.neo4j_client,
        vector_store_factory=make_vector_store
    )
    
    # 3. Execute
    try:
        await use_case.execute(
            DeleteDocumentRequest(
                document_id=document_id,
                tenant_id=tenant_id,
                is_super_admin=is_super_admin
            )
        )
    except LookupError as e:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Error deleting document {document_id}: {e}")
        # In case of other errors, we might still want to return 500 or just generic error
        # Use case swallows non-critical errors (graph/milvus cleanup failure), 
        # so this catches unexpected ones.
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during deletion"
        )
    
    logger.info(f"Document {document_id} deleted")


@router.get(
    "/{document_id}/entities",
    summary="Get Document Entities",
    description="Get entities extracted from a specific document with pagination.",
)
async def get_document_entities(
    document_id: str,
    limit: int = 100,
    offset: int = 0,
    http_request: Request = None,
    session: AsyncSession = Depends(get_db_session),
) -> list[dict[str, Any]]:
    """
    Get entities extracted from a specific document via Neo4j.

    Args:
        document_id: Document UUID
        limit: Maximum number of entities to return (default: 100)
        offset: Number of entities to skip (default: 0)
    """
    # 1. Verify existence in SQL and get tenant_id
    permissions = getattr(http_request.state, "permissions", []) if http_request else []
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
        if http_request:
            tenant_id = _get_tenant_id(http_request)
            query = query.where(Document.tenant_id == tenant_id)
        else:
             # Fallback if request is missing (should not happen in API call)
             pass
    
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # 2. Query Neo4j with pagination
    cypher = """
        MATCH (d:Document {id: $document_id})-[:HAS_CHUNK]->(c:Chunk)
        MATCH (c)-[:MENTIONS]->(e:Entity)
        RETURN DISTINCT e
        ORDER BY e.name
        SKIP $offset
        LIMIT $limit
    """

    try:
        records = await platform.neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id,
                "limit": limit,
                "offset": offset
            }
        )
        # Neo4j Node objects can be converted to dict, but driver returns distinct e as Node.
        # We need to extract properties.
        return [dict(record["e"]) for record in records]
    except Exception as e:
        logger.error(f"Failed to fetch entities for document {document_id}: {e}")
        return []


@router.get(
    "/{document_id}/relationships",
    summary="Get Document Relationships",
    description="Get relationships between entities in this document with pagination.",
)
async def get_document_relationships(
    document_id: str,
    limit: int = 100,
    offset: int = 0,
    http_request: Request = None,
    session: AsyncSession = Depends(get_db_session),
) -> list[dict[str, Any]]:
    """
    Get relationships between entities in this document via Neo4j.

    Optimized query that avoids Cartesian products.

    Args:
        document_id: Document UUID
        limit: Maximum number of relationships to return (default: 100)
        offset: Number of relationships to skip (default: 0)
    """
    # 1. Verify existence
    permissions = getattr(http_request.state, "permissions", []) if http_request else []
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
        if http_request:
            tenant_id = _get_tenant_id(http_request)
            query = query.where(Document.tenant_id == tenant_id)
    
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # 2. Query Neo4j
    # OPTIMIZED: Direct MATCH pattern instead of UNWIND Cartesian product
    # This is O(N) instead of O(NÂ²) where N is the number of entities
    cypher = """
        MATCH (d:Document {id: $document_id})-[:HAS_CHUNK]->(c:Chunk)
        MATCH (c)-[:MENTIONS]->(s:Entity)
        MATCH (s)-[r]->(t:Entity)
        WHERE EXISTS {
              MATCH (d)-[:HAS_CHUNK]->(c2:Chunk)-[:MENTIONS]->(t)
          }
        RETURN DISTINCT {
            source: s.name,
            source_type: s.type,
            target: t.name,
            target_type: t.type,
            type: r.type,
            description: r.description,
            weight: r.weight
        } as rel
        ORDER BY rel.weight DESC
        SKIP $offset
        LIMIT $limit
    """

    try:
        records = await platform.neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id,
                "limit": limit,
                "offset": offset
            }
        )
        return [record["rel"] for record in records]
    except Exception as e:
        logger.error(f"Failed to fetch relationships for document {document_id}: {e}")
        return []


from src.core.ingestion.domain.chunk import Chunk  # noqa: E402


@router.get(
    "/{document_id}/chunks",
    summary="Get Document Chunks",
    description="Get chunks for a specific document.",
    operation_id="get_document_chunks_simple",
)
async def get_document_chunks(
    document_id: str,
    http_request: Request,
    session: AsyncSession = Depends(get_db_session),
) -> list[dict[str, Any]]:
    """
    Get chunks for a document from PostgreSQL.
    """
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    # 1. Verify existence
    # We can join with chunks directly or check doc first.
    # Checking doc first gives better error message.
    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)
        query = query.where(Document.tenant_id == tenant_id)
    
    result = await session.execute(query)
    doc = result.scalars().first()

    if not doc:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )

    # 2. Fetch chunks from Postgres
    chunks_query = (
        select(Chunk)
        .where(Chunk.document_id == document_id)
        .order_by(Chunk.index.asc())
    )
    result = await session.execute(chunks_query)
    chunks = result.scalars().all()

    return [
        {
            "id": chunk.id,
            "index": chunk.index,
            "content": chunk.content,
            "tokens": chunk.tokens,
            "embedding_status": chunk.embedding_status.value
        }
        for chunk in chunks
    ]
@router.get(
    "/{document_id}/similarities",
    summary="Get Document Similarities",
    description="Get similarity relationships between chunks within the document.",
)
async def get_document_similarities(
    document_id: str,
    http_request: Request,
    limit: int = 50,
    offset: int = 0,
    session: AsyncSession = Depends(get_db_session),
) -> list[dict[str, Any]]:
    """
    Get similarity relationships between chunks within the document.
    """
    # Verify document and get tenant
    permissions = getattr(http_request.state, "permissions", [])
    is_super_admin = "super_admin" in permissions

    query = select(Document).where(Document.id == document_id)

    if not is_super_admin:
        tenant_id = _get_tenant_id(http_request)
        query = query.where(Document.tenant_id == tenant_id)
    result = await session.execute(query)
    document = result.scalars().first()

    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document {document_id} not found"
        )
    
    # Query SIMILAR_TO relationships in Neo4j
    # We want chunk text as well to display in frontend
    # Query SIMILAR_TO relationships in Neo4j
    # We fetch chunk IDs from Neo4j, then texts from Postgres (since Neo4j chunks don't have text)
    cypher = """
        MATCH (d:Document {id: $document_id})-[:HAS_CHUNK]->(c1:Chunk)
        MATCH (c1)-[r:SIMILAR_TO]->(c2:Chunk)
        WHERE c1.id < c2.id
        RETURN c1.id as source_id, 
               c2.id as target_id, 
               r.score as score
        ORDER BY r.score DESC
        SKIP $offset
        LIMIT $limit
    """
    
    try:
        # 1. Fetch relations from Neo4j
        records = await platform.neo4j_client.execute_read(
            cypher,
            {
                "document_id": document_id, 
                "offset": offset,
                "limit": limit
            }
        )

        if not records:
            return []

        # 2. Collect unique chunk IDs
        chunk_ids = set()
        for r in records:
            chunk_ids.add(r["source_id"])
            chunk_ids.add(r["target_id"])
        
        # 3. Fetch Chunk text from Postgres
        from src.core.ingestion.domain.chunk import Chunk
        chunk_query = select(Chunk.id, Chunk.content).where(
            Chunk.id.in_(chunk_ids)
        )
        chunk_result = await session.execute(chunk_query)
        chunk_map = {row.id: row.content for row in chunk_result.all()}
        
        # 4. Map back to response
        return [
            {
                "source_id": r.get("source_id"),
                "source_text": (chunk_map.get(r.get("source_id")) or "")[:200] + "...",
                "target_id": r.get("target_id"),
                "target_text": (chunk_map.get(r.get("target_id")) or "")[:200] + "...",
                "score": r.get("score"),
            }
            for r in records
        ]

    except Exception as e:
        logger.warning(f"Failed to fetch similarities for document {document_id}: {e}")
        return []
